{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Parser Benchmark\n",
    "\n",
    "Comparaison des méthodes d'extraction PDF :\n",
    "- **pdfplumber** : Parser actuel (extraction de tableaux)\n",
    "- **Mistral OCR** : Vision model (mistral-ocr-2503)\n",
    "\n",
    "## Objectif\n",
    "Mesurer la précision d'extraction des bulletins scolaires vs ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\Florent\\Documents\\data_science\\chiron\n",
      "PDFs: [WindowsPath('c:/Users/Florent/Documents/data_science/chiron/data/raw/ELEVE_A.pdf'), WindowsPath('c:/Users/Florent/Documents/data_science/chiron/data/raw/ELEVE_B.pdf'), WindowsPath('c:/Users/Florent/Documents/data_science/chiron/data/raw/ELEVE_C.pdf'), WindowsPath('c:/Users/Florent/Documents/data_science/chiron/data/raw/ELEVE_D.pdf')]\n"
     ]
    }
   ],
   "source": [
    "# ruff: noqa: E402\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Auto-détecter project_root\n",
    "current = Path.cwd()\n",
    "while current != current.parent:\n",
    "    if (current / \"pyproject.toml\").exists():\n",
    "        project_root = current\n",
    "        break\n",
    "    current = current.parent\n",
    "\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(project_root / \".env\")\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = project_root / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "GROUND_TRUTH_PATH = DATA_DIR / \"ground_truth\" / \"chiron_ground_truth.json\"\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PDFs: {list(RAW_DIR.glob('*.pdf'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: ['ELEVE_A', 'ELEVE_B', 'ELEVE_C', 'ELEVE_D']\n"
     ]
    }
   ],
   "source": [
    "# Charger ground truth\n",
    "with open(GROUND_TRUTH_PATH, encoding=\"utf-8\") as f:\n",
    "    ground_truth = json.load(f)\n",
    "\n",
    "# Index par eleve_id pour comparaison facile\n",
    "gt_by_eleve = {e[\"eleve_id\"]: e for e in ground_truth[\"eleves\"]}\n",
    "print(f\"Ground truth: {list(gt_by_eleve.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parser pdfplumber (actuel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELEVE_A: 0.07s\n",
      "ELEVE_B: 0.07s\n",
      "ELEVE_C: 0.06s\n",
      "ELEVE_D: 0.07s\n"
     ]
    }
   ],
   "source": [
    "from src.document.bulletin_parser import BulletinParser\n",
    "\n",
    "parser = BulletinParser()\n",
    "\n",
    "pdfplumber_results = {}\n",
    "pdfplumber_times = {}\n",
    "\n",
    "for pdf_path in sorted(RAW_DIR.glob(\"*.pdf\")):\n",
    "    eleve_id = pdf_path.stem  # ELEVE_A, ELEVE_B, etc.\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    try:\n",
    "        eleves = parser.parse(pdf_path)\n",
    "        pdfplumber_results[eleve_id] = eleves[0] if eleves else None\n",
    "    except Exception as e:\n",
    "        pdfplumber_results[eleve_id] = f\"ERROR: {e}\"\n",
    "    pdfplumber_times[eleve_id] = time.perf_counter() - start\n",
    "    \n",
    "    print(f\"{eleve_id}: {pdfplumber_times[eleve_id]:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nom: None\n",
      "Prénom: None\n",
      "Classe: None\n",
      "Matières extraites: 12\n",
      "  - Anglais LV1: 15.21\n",
      "  - Arts Plastiques: 15.0\n",
      "  - EPS: 8.0\n"
     ]
    }
   ],
   "source": [
    "# Afficher un exemple de résultat pdfplumber\n",
    "exemple = pdfplumber_results.get(\"ELEVE_A\")\n",
    "if exemple and not isinstance(exemple, str):\n",
    "    print(f\"Nom: {exemple.nom}\")\n",
    "    print(f\"Prénom: {exemple.prenom}\")\n",
    "    print(f\"Classe: {exemple.classe}\")\n",
    "    print(f\"Matières extraites: {len(exemple.matieres)}\")\n",
    "    for m in exemple.matieres[:3]:\n",
    "        print(f\"  - {m.nom}: {m.moyenne_eleve}\")\n",
    "else:\n",
    "    print(f\"Erreur ou pas de résultat: {exemple}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mistral OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client Mistral initialisé\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "from mistralai import Mistral\n",
    "\n",
    "# Init client\n",
    "mistral_api_key = os.getenv(\"MISTRAL_OCR_API_KEY\")\n",
    "if not mistral_api_key:\n",
    "    raise ValueError(\"MISTRAL_OCR_API_KEY non configurée dans .env\")\n",
    "\n",
    "client = Mistral(api_key=mistral_api_key)\n",
    "print(\"Client Mistral initialisé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_with_mistral_ocr(pdf_path: Path) -> dict:\n",
    "    \"\"\"Extrait le contenu d'un PDF avec Mistral OCR.\n",
    "    \n",
    "    Utilise l'API OCR de Mistral (mistral-ocr-2503).\n",
    "    Ref: https://docs.mistral.ai/capabilities/document/\n",
    "    \"\"\"\n",
    "    # Encoder le PDF en base64\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        pdf_base64 = base64.standard_b64encode(f.read()).decode(\"utf-8\")\n",
    "    \n",
    "    # Appel API OCR\n",
    "    response = client.ocr.process(\n",
    "        model=\"mistral-ocr-2503\",\n",
    "        document={\n",
    "            \"type\": \"document_url\",\n",
    "            \"document_url\": f\"data:application/pdf;base64,{pdf_base64}\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps: 2.73s\n",
      "Type réponse: <class 'mistralai.models.ocrresponse.OCRResponse'>\n",
      "\n",
      "Réponse:\n",
      "pages=[OCRPageObject(index=0, markdown=\"# College Test BULLETIN SCOLAIRE \\n\\nÉlève : ELEVE_A<br>Genre : Fille<br>Absences : 4 demi-journées (justifiées)<br>Engagements : Déléguée titulaire\\n\\n| Matière | Élève / <br> Classe | Appréciation |\\n| :--: | :--: | :--: |\\n| Anglais LV1 | 15.21 / 10.83 | Bons résultats. ELEVE_A fournit un travail régulier et sérieux à la maison tout comme en classe. L'attitude est toujours positive et constructive. Poursuivez ainsi! |\\n| Arts Plastiques | 15.00 / 14.92 | Bon ensemble, bilan satisfaisant, continuez ainsi en restant concentrée. |\\n| EPS | 8.00 / 12.79 | Bilan très insuffisant, ELEVE_A n'a pas réussi à s'orienter avec efficacité. Elle a beaucoup marché et s'est dispersée avec son groupe. Son travail a manqué de sérieux et d'implication dans les défis à relever en course d'orientation. De sincères efforts sont attendus au prochain trimestre. |\\n| Éducation Musicale | 13.00 / 9.53 | C'est très bien quand vous voulez. Votre investissement doit être régulier. |\\n| Espagnol LV2 | 13.83 / 10.76 |  |\\n| Français | 13.21 / 10.12 | Assez bons résultats à l'écrit. La participation orale est satisfaisante. Mais pour progresser encore et consolider les acquis, ELEVE_A doit veiller à rester bien concentrée en classe. |\\n| Histoire-Géographie-EMC | 14.66 / 10.77 | Bon trimestre ! ELEVE_A est une élève impliquée et sérieuse, en témoigne ses résultats. Je l'encourage à conserver cette motivation pour le second trimestre ! |\\n| Latin et Grec | 18.21 / 17.77 | Très bon travail. Élève volontaire et investie. |\\n| Mathématiques | 16.07 / 11.75 | Très bon trimestre. ELEVE_A a de bons acquis, elle est volontaire, sa participation est active et son travail appliqué. Je l'encourage à poursuivre dans cette dynamique, en veillant à ne pas se disperser. |\\n| Physique-Chimie | 15.71 / 12.49 | C'est un très bon trimestre. La participation de ELEVE_A en classe est précieuse. |\\n| SVT | 10.86 / 8.68 | Bilan un peu juste de ELEVE_A. Le travail personnel est encore perfectible comme la participation en classe qui est encore trop timide. |\\n| Technologie | 15.80 / 13.73 | Très bon trimestre, c'est très bien, continuez ainsi. |\\n\\nMoyenne générale : 14.13/20\", images=[], dimensions=OCRPageDimensions(dpi=200, height=2339, width=1654))] model='mistral-ocr-2503' usage_info=OCRUsageInfo(pages_processed=1, doc_size_bytes=3667) document_annotation=None\n"
     ]
    }
   ],
   "source": [
    "# Test sur un seul PDF d'abord\n",
    "test_pdf = RAW_DIR / \"ELEVE_A.pdf\"\n",
    "\n",
    "start = time.perf_counter()\n",
    "result = extract_with_mistral_ocr(test_pdf)\n",
    "elapsed = time.perf_counter() - start\n",
    "\n",
    "print(f\"Temps: {elapsed:.2f}s\")\n",
    "print(f\"Type réponse: {type(result)}\")\n",
    "print(f\"\\nRéponse:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELEVE_A: 2.46s\n",
      "ELEVE_B: 2.70s\n",
      "ELEVE_C: 2.30s\n",
      "ELEVE_D: 2.59s\n"
     ]
    }
   ],
   "source": [
    "# Extraire tous les PDFs avec Mistral OCR\n",
    "mistral_results = {}\n",
    "mistral_times = {}\n",
    "\n",
    "for pdf_path in sorted(RAW_DIR.glob(\"*.pdf\")):\n",
    "    eleve_id = pdf_path.stem\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    try:\n",
    "        result = extract_with_mistral_ocr(pdf_path)\n",
    "        mistral_results[eleve_id] = result\n",
    "    except Exception as e:\n",
    "        mistral_results[eleve_id] = f\"ERROR: {e}\"\n",
    "    mistral_times[eleve_id] = time.perf_counter() - start\n",
    "    \n",
    "    print(f\"{eleve_id}: {mistral_times[eleve_id]:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution (secondes):\n",
      "eleve_id  pdfplumber_time  mistral_time   speedup\n",
      " ELEVE_A         0.068749      2.455649 35.718997\n",
      " ELEVE_B         0.065140      2.701806 41.476981\n",
      " ELEVE_C         0.057187      2.298217 40.187477\n",
      " ELEVE_D         0.068517      2.589566 37.794341\n",
      "\n",
      "Moyenne pdfplumber: 0.06s\n",
      "Moyenne Mistral OCR: 2.51s\n"
     ]
    }
   ],
   "source": [
    "# Tableau comparatif des temps\n",
    "import pandas as pd\n",
    "\n",
    "comparison = []\n",
    "for eleve_id in gt_by_eleve.keys():\n",
    "    comparison.append({\n",
    "        \"eleve_id\": eleve_id,\n",
    "        \"pdfplumber_time\": pdfplumber_times.get(eleve_id, None),\n",
    "        \"mistral_time\": mistral_times.get(eleve_id, None),\n",
    "    })\n",
    "\n",
    "df_times = pd.DataFrame(comparison)\n",
    "df_times[\"speedup\"] = df_times[\"mistral_time\"] / df_times[\"pdfplumber_time\"]\n",
    "print(\"Temps d'exécution (secondes):\")\n",
    "print(df_times.to_string(index=False))\n",
    "print(f\"\\nMoyenne pdfplumber: {df_times['pdfplumber_time'].mean():.2f}s\")\n",
    "print(f\"Moyenne Mistral OCR: {df_times['mistral_time'].mean():.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GROUND TRUTH - ELEVE_A\n",
      "============================================================\n",
      "Genre: F\n",
      "Absences: 4\n",
      "Matières: 12\n",
      "  - ANGLAIS LV1: 15.21 (classe: 10.83)\n",
      "  - ARTS PLASTIQUES: 15.0 (classe: 14.92)\n",
      "  - ED.PHYSIQUE & SPORT: 8.0 (classe: 12.79)\n"
     ]
    }
   ],
   "source": [
    "# Comparer le contenu extrait pour ELEVE_A\n",
    "eleve_id = \"ELEVE_A\"\n",
    "gt = gt_by_eleve[eleve_id]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"GROUND TRUTH - {eleve_id}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Genre: {gt['genre']}\")\n",
    "print(f\"Absences: {gt['absences_demi_journees']}\")\n",
    "print(f\"Matières: {len(gt['matieres'])}\")\n",
    "for m in gt[\"matieres\"][:3]:\n",
    "    print(f\"  - {m['nom']}: {m['moyenne_eleve']} (classe: {m['moyenne_classe']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MISTRAL OCR - ELEVE_A\n",
      "============================================================\n",
      "\n",
      "--- Page 1 ---\n",
      "# College Test BULLETIN SCOLAIRE \n",
      "\n",
      "Élève : ELEVE_A<br>Genre : Fille<br>Absences : 4 demi-journées (justifiées)<br>Engagements : Déléguée titulaire\n",
      "\n",
      "| Matière | Élève / <br> Classe | Appréciation |\n",
      "| :--: | :--: | :--: |\n",
      "| Anglais LV1 | 15.21 / 10.83 | Bons résultats. ELEVE_A fournit un travail régulier et sérieux à la maison tout comme en classe. L'attitude est toujours positive et constructive. Poursuivez ainsi! |\n",
      "| Arts Plastiques | 15.00 / 14.92 | Bon ensemble, bilan satisfaisant, continuez ainsi en restant concentrée. |\n",
      "| EPS | 8.00 / 12.79 | Bilan très insuffisant, ELEVE_A n'a pas réussi à s'orienter avec efficacité. Elle a beaucoup marché et s'est dispersée avec son groupe. Son travail a manqué de sérieux et d'implication dans les défis à relever en course d'orientation. De sincères efforts sont attendus au prochain trimestre. |\n",
      "| Éducation Musicale | 13.00 / 9.53 | C'est très bien quand vous voulez. Votre investissement doit être régulier. |\n",
      "| Espagnol LV2 | 13.83 / 10.76 |  |\n",
      "| Français | 13.21 / 10.12 | Assez bons résultats à l'écrit. La participation orale est satisfaisante. Mais pour progresser encore et consolider les acquis, ELEVE_A doit veiller à rester bien concentrée en classe. |\n",
      "| Histoire-Géographie-EMC | 14.66 / 10.77 | Bon trimestre ! ELEVE_A est une élève impliquée et sérieuse, en témoigne ses résultats. Je l'encourage à conserver cette motivation pour le second trimestre ! |\n",
      "| Latin et Grec | 18.21 / 17.77 | Très bon travail. Élève volontaire et investie. |\n",
      "| Mathématiques | 16.07 / 11.75 | Très bon trimestre. ELEVE_A a de bons acquis, elle est volontaire, sa participation est active et son travail appliqué. Je l'encourage à poursuivre dans cette dynamique, en veillant à ne pas se disperser. |\n",
      "| Physique-Chimie | 15.71 / 12.49 | C'est un très bon trimestre. La participation de ELEVE_A en classe est précieuse. |\n",
      "| SVT | 10.86 / 8.68 | Bilan un peu juste de ELEVE_A. Le travail personnel est encore perfectible comme la participation en classe qui est encore trop timide. |\n",
      "| Technologie | 15.80 / 13.73 | Très bon trimestre, c'est très bien, continuez ainsi. |\n",
      "\n",
      "Moyenne générale : 14.13/20\n"
     ]
    }
   ],
   "source": [
    "# Afficher le texte brut extrait par Mistral OCR (complet)\n",
    "print(\"=\" * 60)\n",
    "print(f\"MISTRAL OCR - {eleve_id}\")\n",
    "print(\"=\" * 60)\n",
    "mistral_result = mistral_results.get(eleve_id)\n",
    "if mistral_result and not isinstance(mistral_result, str):\n",
    "    if hasattr(mistral_result, 'pages'):\n",
    "        for i, page in enumerate(mistral_result.pages):\n",
    "            print(f\"\\n--- Page {i+1} ---\")\n",
    "            print(page.markdown)  # Afficher tout le contenu\n",
    "    else:\n",
    "        print(mistral_result)\n",
    "else:\n",
    "    print(f\"Erreur: {mistral_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparaison cellule par cellule\n",
    "\n",
    "Parser le markdown de Mistral OCR et comparer avec le ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Élève: ELEVE_A\n",
      "Genre: Fille\n",
      "Absences: 4\n",
      "Engagements: Déléguée titulaire\n",
      "Moyenne générale: 14.13\n",
      "Matières: 12\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def parse_mistral_markdown(markdown: str) -> dict:\n",
    "    \"\"\"Parse le markdown de Mistral OCR en structure de données.\"\"\"\n",
    "    result = {\n",
    "        \"eleve\": None,\n",
    "        \"genre\": None,\n",
    "        \"absences\": None,\n",
    "        \"engagements\": None,\n",
    "        \"matieres\": [],\n",
    "        \"moyenne_generale\": None,\n",
    "    }\n",
    "    \n",
    "    # Extraire les métadonnées (format: \"Élève : ELEVE_A<br>Genre : Fille...\")\n",
    "    eleve_match = re.search(r\"Élève\\s*:\\s*(\\w+)\", markdown)\n",
    "    if eleve_match:\n",
    "        result[\"eleve\"] = eleve_match.group(1)\n",
    "    \n",
    "    genre_match = re.search(r\"Genre\\s*:\\s*(\\w+)\", markdown)\n",
    "    if genre_match:\n",
    "        result[\"genre\"] = genre_match.group(1)\n",
    "    \n",
    "    absences_match = re.search(r\"Absences\\s*:\\s*(\\d+)\", markdown)\n",
    "    if absences_match:\n",
    "        result[\"absences\"] = int(absences_match.group(1))\n",
    "    \n",
    "    engagements_match = re.search(r\"Engagements\\s*:\\s*([^<\\n]+)\", markdown)\n",
    "    if engagements_match:\n",
    "        result[\"engagements\"] = engagements_match.group(1).strip()\n",
    "    \n",
    "    # Extraire les lignes du tableau markdown\n",
    "    # Format: | Matière | Élève / Classe | Appréciation |\n",
    "    table_pattern = r\"\\|\\s*([^|]+)\\s*\\|\\s*([^|]+)\\s*\\|\\s*([^|]*)\\s*\\|\"\n",
    "    rows = re.findall(table_pattern, markdown)\n",
    "    \n",
    "    for row in rows:\n",
    "        matiere, notes, appreciation = [cell.strip() for cell in row]\n",
    "        \n",
    "        # Skip header rows\n",
    "        if matiere in (\"Matière\", \":--:\") or \"Élève\" in matiere:\n",
    "            continue\n",
    "        \n",
    "        # Parser les notes (format: \"15.21 / 10.83\")\n",
    "        notes_match = re.search(r\"([\\d.]+)\\s*/\\s*([\\d.]+)\", notes)\n",
    "        moyenne_eleve = float(notes_match.group(1)) if notes_match else None\n",
    "        moyenne_classe = float(notes_match.group(2)) if notes_match else None\n",
    "        \n",
    "        result[\"matieres\"].append({\n",
    "            \"nom\": matiere,\n",
    "            \"moyenne_eleve\": moyenne_eleve,\n",
    "            \"moyenne_classe\": moyenne_classe,\n",
    "            \"appreciation\": appreciation,\n",
    "        })\n",
    "    \n",
    "    # Moyenne générale\n",
    "    moy_match = re.search(r\"Moyenne générale\\s*:\\s*([\\d.]+)\", markdown)\n",
    "    if moy_match:\n",
    "        result[\"moyenne_generale\"] = float(moy_match.group(1))\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Test du parser\n",
    "test_markdown = mistral_results[\"ELEVE_A\"].pages[0].markdown\n",
    "parsed = parse_mistral_markdown(test_markdown)\n",
    "\n",
    "print(f\"Élève: {parsed['eleve']}\")\n",
    "print(f\"Genre: {parsed['genre']}\")\n",
    "print(f\"Absences: {parsed['absences']}\")\n",
    "print(f\"Engagements: {parsed['engagements']}\")\n",
    "print(f\"Moyenne générale: {parsed['moyenne_generale']}\")\n",
    "print(f\"Matières: {len(parsed['matieres'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparaison Mistral OCR vs Ground Truth pour ELEVE_A:\n",
      "\n",
      "            matiere_ocr  moy_ocr  moy_gt moy_ok classe_ok app_ok\n",
      "            Anglais LV1    15.21   15.21      ✅         ✅      ✅\n",
      "        Arts Plastiques    15.00   15.00      ✅         ✅      ✅\n",
      "                    EPS     8.00    8.00      ✅         ✅      ✅\n",
      "     Éducation Musicale    13.00   13.00      ✅         ✅      ✅\n",
      "           Espagnol LV2    13.83   13.83      ✅         ✅      ✅\n",
      "               Français    13.21   13.21      ✅         ✅      ✅\n",
      "Histoire-Géographie-EMC    14.66   14.66      ✅         ✅      ✅\n",
      "          Latin et Grec    18.21   18.21      ✅         ✅      ✅\n",
      "          Mathématiques    16.07   16.07      ✅         ✅      ✅\n",
      "        Physique-Chimie    15.71   15.71      ✅         ✅      ✅\n",
      "                    SVT    10.86   10.86      ✅         ✅      ✅\n",
      "            Technologie    15.80   15.80      ✅         ✅      ✅\n"
     ]
    }
   ],
   "source": [
    "# Mapping des noms de matières (PDF simplifié vs Ground Truth)\n",
    "MATIERE_MAPPING = {\n",
    "    \"Anglais LV1\": \"ANGLAIS LV1\",\n",
    "    \"Arts Plastiques\": \"ARTS PLASTIQUES\",\n",
    "    \"EPS\": \"ED.PHYSIQUE & SPORT\",\n",
    "    \"Éducation Musicale\": \"EDUCATION MUSICALE\",\n",
    "    \"Espagnol LV2\": \"ESPAGNOL LV2\",\n",
    "    \"Français\": \"FRANCAIS\",\n",
    "    \"Histoire-Géographie-EMC\": \"HISTOIRE-GEOGRAPHIE-EMC\",\n",
    "    \"Latin et Grec\": \"LCA LATIN ET GREC\",\n",
    "    \"Mathématiques\": \"MATHEMATIQUES\",\n",
    "    \"Physique-Chimie\": \"PHYSIQUE-CHIMIE\",\n",
    "    \"SVT\": \"SCIENCES VIE & TERRE\",\n",
    "    \"Technologie\": \"TECHNOLOGIE\",\n",
    "    \"Italien LV2\": \"ITALIEN LV2\",\n",
    "}\n",
    "\n",
    "\n",
    "def compare_with_ground_truth(parsed: dict, gt: dict) -> pd.DataFrame:\n",
    "    \"\"\"Compare les données parsées avec le ground truth.\"\"\"\n",
    "    comparisons = []\n",
    "    \n",
    "    # Index GT par nom de matière\n",
    "    gt_matieres = {m[\"nom\"]: m for m in gt[\"matieres\"]}\n",
    "    \n",
    "    for m in parsed[\"matieres\"]:\n",
    "        nom_ocr = m[\"nom\"]\n",
    "        nom_gt = MATIERE_MAPPING.get(nom_ocr, nom_ocr.upper())\n",
    "        gt_matiere = gt_matieres.get(nom_gt, {})\n",
    "        \n",
    "        # Comparer moyenne élève\n",
    "        ocr_moy = m[\"moyenne_eleve\"]\n",
    "        gt_moy = gt_matiere.get(\"moyenne_eleve\")\n",
    "        moy_match = ocr_moy == gt_moy if ocr_moy and gt_moy else None\n",
    "        \n",
    "        # Comparer moyenne classe\n",
    "        ocr_classe = m[\"moyenne_classe\"]\n",
    "        gt_classe = gt_matiere.get(\"moyenne_classe\")\n",
    "        classe_match = ocr_classe == gt_classe if ocr_classe and gt_classe else None\n",
    "        \n",
    "        # Comparer appréciation (normaliser les espaces)\n",
    "        ocr_app = \" \".join(m[\"appreciation\"].split()) if m[\"appreciation\"] else \"\"\n",
    "        gt_app = \" \".join(gt_matiere.get(\"appreciation\", \"\").split())\n",
    "        app_match = ocr_app == gt_app\n",
    "        \n",
    "        comparisons.append({\n",
    "            \"matiere_ocr\": nom_ocr,\n",
    "            \"matiere_gt\": nom_gt,\n",
    "            \"moy_ocr\": ocr_moy,\n",
    "            \"moy_gt\": gt_moy,\n",
    "            \"moy_ok\": \"✅\" if moy_match else \"❌\" if moy_match is False else \"⚠️\",\n",
    "            \"classe_ocr\": ocr_classe,\n",
    "            \"classe_gt\": gt_classe,\n",
    "            \"classe_ok\": \"✅\" if classe_match else \"❌\" if classe_match is False else \"⚠️\",\n",
    "            \"app_ok\": \"✅\" if app_match else \"❌\",\n",
    "            \"app_len_ocr\": len(ocr_app),\n",
    "            \"app_len_gt\": len(gt_app),\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(comparisons)\n",
    "\n",
    "\n",
    "# Comparer ELEVE_A\n",
    "df_comparison = compare_with_ground_truth(parsed, gt_by_eleve[\"ELEVE_A\"])\n",
    "print(\"Comparaison Mistral OCR vs Ground Truth pour ELEVE_A:\\n\")\n",
    "print(df_comparison[[\"matiere_ocr\", \"moy_ocr\", \"moy_gt\", \"moy_ok\", \"classe_ok\", \"app_ok\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STATISTIQUES GLOBALES - Mistral OCR vs Ground Truth\n",
      "============================================================\n",
      "\n",
      "Total matières analysées: 46\n",
      "Moyennes élève correctes: 46/46 (100.0%)\n",
      "Moyennes classe correctes: 46/46 (100.0%)\n",
      "Appréciations identiques: 43/46 (93.5%)\n"
     ]
    }
   ],
   "source": [
    "# Comparer tous les élèves\n",
    "all_comparisons = []\n",
    "\n",
    "for eleve_id in gt_by_eleve.keys():\n",
    "    mistral_result = mistral_results.get(eleve_id)\n",
    "    if mistral_result and not isinstance(mistral_result, str):\n",
    "        parsed = parse_mistral_markdown(mistral_result.pages[0].markdown)\n",
    "        df = compare_with_ground_truth(parsed, gt_by_eleve[eleve_id])\n",
    "        df[\"eleve_id\"] = eleve_id\n",
    "        all_comparisons.append(df)\n",
    "\n",
    "df_all = pd.concat(all_comparisons, ignore_index=True)\n",
    "\n",
    "# Statistiques globales\n",
    "total = len(df_all)\n",
    "moy_ok = (df_all[\"moy_ok\"] == \"✅\").sum()\n",
    "classe_ok = (df_all[\"classe_ok\"] == \"✅\").sum()\n",
    "app_ok = (df_all[\"app_ok\"] == \"✅\").sum()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STATISTIQUES GLOBALES - Mistral OCR vs Ground Truth\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal matières analysées: {total}\")\n",
    "print(f\"Moyennes élève correctes: {moy_ok}/{total} ({100*moy_ok/total:.1f}%)\")\n",
    "print(f\"Moyennes classe correctes: {classe_ok}/{total} ({100*classe_ok/total:.1f}%)\")\n",
    "print(f\"Appréciations identiques: {app_ok}/{total} ({100*app_ok/total:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appréciations différentes (3):\n",
      "\n",
      "eleve_id             matiere_ocr  app_len_ocr  app_len_gt\n",
      " ELEVE_B             Anglais LV1          218         219\n",
      " ELEVE_B                     EPS           90          91\n",
      " ELEVE_C Histoire-Géographie-EMC           56          56\n"
     ]
    }
   ],
   "source": [
    "# Détail des appréciations qui ne matchent pas\n",
    "df_errors = df_all[df_all[\"app_ok\"] == \"❌\"][[\"eleve_id\", \"matiere_ocr\", \"app_len_ocr\", \"app_len_gt\"]]\n",
    "print(f\"Appréciations différentes ({len(df_errors)}):\\n\")\n",
    "print(df_errors.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export des extractions pour comparaison\n",
    "\n",
    "Sauvegarder les extractions en markdown pour faciliter la comparaison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ELEVE_A exporté\n",
      "✓ ELEVE_B exporté\n",
      "✓ ELEVE_C exporté\n",
      "✓ ELEVE_D exporté\n",
      "\n",
      "Fichiers exportés dans: c:\\Users\\Florent\\Documents\\data_science\\chiron\\data\\processed\\benchmark-extraction\n"
     ]
    }
   ],
   "source": [
    "# Dossier de sortie\n",
    "BENCHMARK_DIR = DATA_DIR / \"processed\" / \"benchmark-extraction\"\n",
    "BENCHMARK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def export_ground_truth_md(gt: dict, output_path: Path):\n",
    "    \"\"\"Exporte le ground truth en markdown.\"\"\"\n",
    "    lines = [\n",
    "        f\"# Ground Truth - {gt['eleve_id']}\",\n",
    "        \"\",\n",
    "        f\"**Genre:** {gt['genre']}\",\n",
    "        f\"**Absences:** {gt['absences_demi_journees']} demi-journées\",\n",
    "        f\"**Engagements:** {', '.join(gt.get('engagements', [])) or 'Aucun'}\",\n",
    "        \"\",\n",
    "        \"## Matières\",\n",
    "        \"\",\n",
    "        \"| Matière | Élève | Classe | Appréciation |\",\n",
    "        \"|---------|-------|--------|--------------|\",\n",
    "    ]\n",
    "    \n",
    "    for m in gt[\"matieres\"]:\n",
    "        app = m.get(\"appreciation\", \"\").replace(\"|\", \"\\\\|\").replace(\"\\n\", \" \")\n",
    "        lines.append(f\"| {m['nom']} | {m['moyenne_eleve']} | {m['moyenne_classe']} | {app} |\")\n",
    "    \n",
    "    output_path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def export_mistral_ocr_md(parsed: dict, output_path: Path):\n",
    "    \"\"\"Exporte l'extraction Mistral OCR en markdown.\"\"\"\n",
    "    lines = [\n",
    "        f\"# Mistral OCR - {parsed['eleve']}\",\n",
    "        \"\",\n",
    "        f\"**Genre:** {parsed['genre']}\",\n",
    "        f\"**Absences:** {parsed['absences']} demi-journées\",\n",
    "        f\"**Engagements:** {parsed['engagements'] or 'Aucun'}\",\n",
    "        \"\",\n",
    "        \"## Matières\",\n",
    "        \"\",\n",
    "        \"| Matière | Élève | Classe | Appréciation |\",\n",
    "        \"|---------|-------|--------|--------------|\",\n",
    "    ]\n",
    "    \n",
    "    for m in parsed[\"matieres\"]:\n",
    "        app = m.get(\"appreciation\", \"\").replace(\"|\", \"\\\\|\").replace(\"\\n\", \" \")\n",
    "        lines.append(f\"| {m['nom']} | {m['moyenne_eleve']} | {m['moyenne_classe']} | {app} |\")\n",
    "    \n",
    "    output_path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def export_pdfplumber_md(result, output_path: Path):\n",
    "    \"\"\"Exporte l'extraction pdfplumber en markdown.\"\"\"\n",
    "    if isinstance(result, str):\n",
    "        output_path.write_text(f\"# Erreur\\n\\n{result}\", encoding=\"utf-8\")\n",
    "        return\n",
    "    \n",
    "    lines = [\n",
    "        f\"# pdfplumber - {result.nom or 'N/A'}\",\n",
    "        \"\",\n",
    "        f\"**Genre:** {result.genre or 'N/A'}\",\n",
    "        f\"**Absences:** {result.absences_demi_journees or 'N/A'} demi-journées\",\n",
    "        f\"**Engagements:** {', '.join(result.engagements) if result.engagements else 'Aucun'}\",\n",
    "        \"\",\n",
    "        \"## Matières\",\n",
    "        \"\",\n",
    "        \"| Matière | Élève | Classe | Appréciation |\",\n",
    "        \"|---------|-------|--------|--------------|\",\n",
    "    ]\n",
    "    \n",
    "    for m in result.matieres:\n",
    "        app = (m.appreciation or \"\").replace(\"|\", \"\\\\|\").replace(\"\\n\", \" \")\n",
    "        lines.append(f\"| {m.nom} | {m.moyenne_eleve} | {m.moyenne_classe} | {app} |\")\n",
    "    \n",
    "    output_path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# Exporter pour tous les élèves\n",
    "for eleve_id in gt_by_eleve.keys():\n",
    "    # Ground truth\n",
    "    export_ground_truth_md(\n",
    "        gt_by_eleve[eleve_id],\n",
    "        BENCHMARK_DIR / f\"{eleve_id}_ground_truth.md\"\n",
    "    )\n",
    "    \n",
    "    # Mistral OCR\n",
    "    mistral_result = mistral_results.get(eleve_id)\n",
    "    if mistral_result and not isinstance(mistral_result, str):\n",
    "        parsed = parse_mistral_markdown(mistral_result.pages[0].markdown)\n",
    "        export_mistral_ocr_md(parsed, BENCHMARK_DIR / f\"{eleve_id}_mistral_ocr.md\")\n",
    "    \n",
    "    # pdfplumber\n",
    "    pdf_result = pdfplumber_results.get(eleve_id)\n",
    "    export_pdfplumber_md(pdf_result, BENCHMARK_DIR / f\"{eleve_id}_pdfplumber.md\")\n",
    "    \n",
    "    print(f\"✓ {eleve_id} exporté\")\n",
    "\n",
    "print(f\"\\nFichiers exportés dans: {BENCHMARK_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple de différence: ELEVE_B - Anglais LV1\n",
      "\n",
      "OCR (218 chars):\n",
      "Bons résultats à l'écrit temis par une moyenne orale faible due à un accident au dernier contrôle. Je suis sûre que ELEVE_B saura y remédier au trimestre prochain en participant davantage en classe. Je compte sur elle.\n",
      "\n",
      "GT (219 chars):\n",
      "Bons résultats à l'écrit ternis par une moyenne orale faible due à un accident au dernier contrôle. Je suis sûre que ELEVE_B saura y remédier au trimestre prochain en participant davantage en classe. Je compte sur elle.\n"
     ]
    }
   ],
   "source": [
    "# Exemple de différence d'appréciation (si existe)\n",
    "if len(df_errors) > 0:\n",
    "    sample = df_errors.iloc[0]\n",
    "    eleve_id = sample[\"eleve_id\"]\n",
    "    matiere = sample[\"matiere_ocr\"]\n",
    "    \n",
    "    # Récupérer les appréciations\n",
    "    parsed = parse_mistral_markdown(mistral_results[eleve_id].pages[0].markdown)\n",
    "    ocr_app = next((m[\"appreciation\"] for m in parsed[\"matieres\"] if m[\"nom\"] == matiere), \"\")\n",
    "    \n",
    "    nom_gt = MATIERE_MAPPING.get(matiere, matiere.upper())\n",
    "    gt_app = next((m[\"appreciation\"] for m in gt_by_eleve[eleve_id][\"matieres\"] if m[\"nom\"] == nom_gt), \"\")\n",
    "    \n",
    "    print(f\"Exemple de différence: {eleve_id} - {matiere}\\n\")\n",
    "    print(f\"OCR ({len(ocr_app)} chars):\\n{ocr_app}\\n\")\n",
    "    print(f\"GT ({len(gt_app)} chars):\\n{gt_app}\")\n",
    "else:\n",
    "    print(\"Toutes les appréciations sont identiques!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparaison pdfplumber vs Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparaison pdfplumber vs Ground Truth pour ELEVE_A:\n",
      "\n",
      "            matiere_pdf  moy_pdf  moy_gt moy_ok classe_ok app_ok\n",
      "            Anglais LV1    15.21   15.21      ✅         ✅      ❌\n",
      "        Arts Plastiques    15.00   15.00      ✅         ✅      ❌\n",
      "                    EPS     8.00    8.00      ✅         ✅      ❌\n",
      "     Éducation Musicale    13.00   13.00      ✅         ✅      ❌\n",
      "           Espagnol LV2    13.83   13.83      ✅         ✅      ✅\n",
      "               Français    13.21   13.21      ✅         ✅      ❌\n",
      "Histoire-Géographie-EMC    14.66   14.66      ✅         ✅      ❌\n",
      "          Latin et Grec    18.21   18.21      ✅         ✅      ❌\n",
      "          Mathématiques    16.07   16.07      ✅         ✅      ❌\n",
      "        Physique-Chimie    15.71   15.71      ✅         ✅      ❌\n",
      "                    SVT    10.86   10.86      ✅         ✅      ❌\n",
      "            Technologie    15.80   15.80      ✅         ✅      ❌\n"
     ]
    }
   ],
   "source": [
    "def compare_pdfplumber_with_gt(pdfplumber_result, gt: dict) -> pd.DataFrame:\n",
    "    \"\"\"Compare les données pdfplumber avec le ground truth.\"\"\"\n",
    "    comparisons = []\n",
    "    \n",
    "    if isinstance(pdfplumber_result, str):  # Error\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Index GT par nom de matière\n",
    "    gt_matieres = {m[\"nom\"]: m for m in gt[\"matieres\"]}\n",
    "    \n",
    "    for m in pdfplumber_result.matieres:\n",
    "        nom_pdf = m.nom\n",
    "        nom_gt = MATIERE_MAPPING.get(nom_pdf, nom_pdf.upper())\n",
    "        gt_matiere = gt_matieres.get(nom_gt, {})\n",
    "        \n",
    "        # Comparer moyenne élève\n",
    "        pdf_moy = m.moyenne_eleve\n",
    "        gt_moy = gt_matiere.get(\"moyenne_eleve\")\n",
    "        moy_match = pdf_moy == gt_moy if pdf_moy and gt_moy else None\n",
    "        \n",
    "        # Comparer moyenne classe\n",
    "        pdf_classe = m.moyenne_classe\n",
    "        gt_classe = gt_matiere.get(\"moyenne_classe\")\n",
    "        classe_match = pdf_classe == gt_classe if pdf_classe and gt_classe else None\n",
    "        \n",
    "        # Comparer appréciation\n",
    "        pdf_app = \" \".join(m.appreciation.split()) if m.appreciation else \"\"\n",
    "        gt_app = \" \".join(gt_matiere.get(\"appreciation\", \"\").split())\n",
    "        app_match = pdf_app == gt_app\n",
    "        \n",
    "        comparisons.append({\n",
    "            \"matiere_pdf\": nom_pdf,\n",
    "            \"matiere_gt\": nom_gt,\n",
    "            \"moy_pdf\": pdf_moy,\n",
    "            \"moy_gt\": gt_moy,\n",
    "            \"moy_ok\": \"✅\" if moy_match else \"❌\" if moy_match is False else \"⚠️\",\n",
    "            \"classe_pdf\": pdf_classe,\n",
    "            \"classe_gt\": gt_classe,\n",
    "            \"classe_ok\": \"✅\" if classe_match else \"❌\" if classe_match is False else \"⚠️\",\n",
    "            \"app_ok\": \"✅\" if app_match else \"❌\",\n",
    "            \"app_len_pdf\": len(pdf_app),\n",
    "            \"app_len_gt\": len(gt_app),\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(comparisons)\n",
    "\n",
    "\n",
    "# Comparer ELEVE_A avec pdfplumber\n",
    "df_pdf_comparison = compare_pdfplumber_with_gt(pdfplumber_results[\"ELEVE_A\"], gt_by_eleve[\"ELEVE_A\"])\n",
    "print(\"Comparaison pdfplumber vs Ground Truth pour ELEVE_A:\\n\")\n",
    "print(df_pdf_comparison[[\"matiere_pdf\", \"moy_pdf\", \"moy_gt\", \"moy_ok\", \"classe_ok\", \"app_ok\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STATISTIQUES GLOBALES - pdfplumber vs Ground Truth\n",
      "============================================================\n",
      "\n",
      "Total matières analysées: 46\n",
      "Moyennes élève correctes: 46/46 (100.0%)\n",
      "Moyennes classe correctes: 46/46 (100.0%)\n",
      "Appréciations identiques: 3/46 (6.5%)\n"
     ]
    }
   ],
   "source": [
    "# Statistiques globales pdfplumber\n",
    "all_pdf_comparisons = []\n",
    "\n",
    "for eleve_id in gt_by_eleve.keys():\n",
    "    pdf_result = pdfplumber_results.get(eleve_id)\n",
    "    if pdf_result and not isinstance(pdf_result, str):\n",
    "        df = compare_pdfplumber_with_gt(pdf_result, gt_by_eleve[eleve_id])\n",
    "        df[\"eleve_id\"] = eleve_id\n",
    "        all_pdf_comparisons.append(df)\n",
    "\n",
    "df_all_pdf = pd.concat(all_pdf_comparisons, ignore_index=True)\n",
    "\n",
    "total_pdf = len(df_all_pdf)\n",
    "moy_ok_pdf = (df_all_pdf[\"moy_ok\"] == \"✅\").sum()\n",
    "classe_ok_pdf = (df_all_pdf[\"classe_ok\"] == \"✅\").sum()\n",
    "app_ok_pdf = (df_all_pdf[\"app_ok\"] == \"✅\").sum()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STATISTIQUES GLOBALES - pdfplumber vs Ground Truth\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal matières analysées: {total_pdf}\")\n",
    "print(f\"Moyennes élève correctes: {moy_ok_pdf}/{total_pdf} ({100*moy_ok_pdf/total_pdf:.1f}%)\")\n",
    "print(f\"Moyennes classe correctes: {classe_ok_pdf}/{total_pdf} ({100*classe_ok_pdf/total_pdf:.1f}%)\")\n",
    "print(f\"Appréciations identiques: {app_ok_pdf}/{total_pdf} ({100*app_ok_pdf/total_pdf:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Résumé comparatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RÉSUMÉ BENCHMARK - Mistral OCR vs pdfplumber\n",
      "======================================================================\n",
      "                 Métrique pdfplumber Mistral OCR\n",
      "          Temps moyen (s)       0.06        2.51\n",
      " Moyennes élève correctes     100.0%      100.0%\n",
      "Moyennes classe correctes     100.0%      100.0%\n",
      " Appréciations identiques       6.5%       93.5%\n",
      "\n",
      "======================================================================\n",
      "CONCLUSION\n",
      "======================================================================\n",
      "\n",
      "- Mistral OCR est ~30x plus lent mais retourne du markdown structuré\n",
      "- Les deux méthodes extraient correctement les notes numériques\n",
      "- Vérifier les différences d'appréciations (ponctuation, espaces)\n",
      "- Mistral OCR ne nécessite pas de logique de parsing de tableaux spécifique\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Résumé comparatif final\n",
    "print(\"=\" * 70)\n",
    "print(\"RÉSUMÉ BENCHMARK - Mistral OCR vs pdfplumber\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"Métrique\": [\n",
    "        \"Temps moyen (s)\",\n",
    "        \"Moyennes élève correctes\",\n",
    "        \"Moyennes classe correctes\", \n",
    "        \"Appréciations identiques\",\n",
    "    ],\n",
    "    \"pdfplumber\": [\n",
    "        f\"{df_times['pdfplumber_time'].mean():.2f}\",\n",
    "        f\"{100*moy_ok_pdf/total_pdf:.1f}%\",\n",
    "        f\"{100*classe_ok_pdf/total_pdf:.1f}%\",\n",
    "        f\"{100*app_ok_pdf/total_pdf:.1f}%\",\n",
    "    ],\n",
    "    \"Mistral OCR\": [\n",
    "        f\"{df_times['mistral_time'].mean():.2f}\",\n",
    "        f\"{100*moy_ok/total:.1f}%\",\n",
    "        f\"{100*classe_ok/total:.1f}%\",\n",
    "        f\"{100*app_ok/total:.1f}%\",\n",
    "    ],\n",
    "})\n",
    "\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "- Mistral OCR est ~30x plus lent mais retourne du markdown structuré\n",
    "- Les deux méthodes extraient correctement les notes numériques\n",
    "- Vérifier les différences d'appréciations (ponctuation, espaces)\n",
    "- Mistral OCR ne nécessite pas de logique de parsing de tableaux spécifique\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chiron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
