{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Parser Benchmark\n",
    "\n",
    "Comparaison des méthodes d'extraction PDF :\n",
    "- **pdfplumber** : Parser actuel (extraction de tableaux)\n",
    "- **Mistral OCR** : Vision model (mistral-ocr-2503)\n",
    "\n",
    "## Objectif\n",
    "Mesurer la précision d'extraction des bulletins scolaires vs ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\Florent\\Documents\\data_science\\chiron\n",
      "PDFs: [WindowsPath('c:/Users/Florent/Documents/data_science/chiron/data/raw/ELEVE_A.pdf'), WindowsPath('c:/Users/Florent/Documents/data_science/chiron/data/raw/ELEVE_B.pdf'), WindowsPath('c:/Users/Florent/Documents/data_science/chiron/data/raw/ELEVE_C.pdf'), WindowsPath('c:/Users/Florent/Documents/data_science/chiron/data/raw/ELEVE_D.pdf')]\n"
     ]
    }
   ],
   "source": [
    "# ruff: noqa: E402\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Auto-détecter project_root\n",
    "current = Path.cwd()\n",
    "while current != current.parent:\n",
    "    if (current / \"pyproject.toml\").exists():\n",
    "        project_root = current\n",
    "        break\n",
    "    current = current.parent\n",
    "\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(project_root / \".env\")\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = project_root / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "GROUND_TRUTH_PATH = DATA_DIR / \"ground_truth\" / \"chiron_ground_truth.json\"\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PDFs: {list(RAW_DIR.glob('*.pdf'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: ['ELEVE_A', 'ELEVE_B', 'ELEVE_C', 'ELEVE_D']\n"
     ]
    }
   ],
   "source": [
    "# Charger ground truth\n",
    "with open(GROUND_TRUTH_PATH, encoding=\"utf-8\") as f:\n",
    "    ground_truth = json.load(f)\n",
    "\n",
    "# Index par eleve_id pour comparaison facile\n",
    "gt_by_eleve = {e[\"eleve_id\"]: e for e in ground_truth[\"eleves\"]}\n",
    "print(f\"Ground truth: {list(gt_by_eleve.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parser pdfplumber (actuel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELEVE_A: 0.06s\n",
      "ELEVE_B: 0.06s\n",
      "ELEVE_C: 0.06s\n",
      "ELEVE_D: 0.07s\n"
     ]
    }
   ],
   "source": [
    "from src.document.bulletin_parser import BulletinParser\n",
    "\n",
    "parser = BulletinParser()\n",
    "\n",
    "pdfplumber_results = {}\n",
    "pdfplumber_times = {}\n",
    "\n",
    "for pdf_path in sorted(RAW_DIR.glob(\"*.pdf\")):\n",
    "    eleve_id = pdf_path.stem  # ELEVE_A, ELEVE_B, etc.\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    try:\n",
    "        eleves = parser.parse(pdf_path)\n",
    "        pdfplumber_results[eleve_id] = eleves[0] if eleves else None\n",
    "    except Exception as e:\n",
    "        pdfplumber_results[eleve_id] = f\"ERROR: {e}\"\n",
    "    pdfplumber_times[eleve_id] = time.perf_counter() - start\n",
    "\n",
    "    print(f\"{eleve_id}: {pdfplumber_times[eleve_id]:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nom: None\n",
      "Prénom: None\n",
      "Classe: None\n",
      "Matières extraites: 0\n"
     ]
    }
   ],
   "source": [
    "# Afficher un exemple de résultat pdfplumber\n",
    "exemple = pdfplumber_results.get(\"ELEVE_A\")\n",
    "if exemple and not isinstance(exemple, str):\n",
    "    print(f\"Nom: {exemple.nom}\")\n",
    "    print(f\"Prénom: {exemple.prenom}\")\n",
    "    print(f\"Classe: {exemple.classe}\")\n",
    "    print(f\"Matières extraites: {len(exemple.matieres)}\")\n",
    "    for m in exemple.matieres[:]:\n",
    "        print(f\"  - {m.nom}: {m.moyenne_eleve}\")\n",
    "else:\n",
    "    print(f\"Erreur ou pas de résultat: {exemple}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mistral OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client Mistral initialisé\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "from mistralai import Mistral\n",
    "\n",
    "# Init client\n",
    "mistral_api_key = os.getenv(\"MISTRAL_OCR_API_KEY\")\n",
    "if not mistral_api_key:\n",
    "    raise ValueError(\"MISTRAL_OCR_API_KEY non configurée dans .env\")\n",
    "\n",
    "client = Mistral(api_key=mistral_api_key)\n",
    "mistral_model = \"mistral-ocr-latest\"\n",
    "print(\"Client Mistral initialisé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_pdf(pdf_path):\n",
    "    \"\"\"Encode the pdf to base64.\"\"\"\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as pdf_file:\n",
    "            return base64.b64encode(pdf_file.read()).decode('utf-8')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {pdf_path} was not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_with_mistral_ocr(pdf_path: Path) -> dict:\n",
    "    \"\"\"Extrait le contenu d'un PDF avec Mistral OCR.\"\"\"\n",
    "    # Getting the base64 string\n",
    "    base64_pdf = encode_pdf(pdf_path)\n",
    "\n",
    "    # Call the OCR API\n",
    "    pdf_response = client.ocr.process(\n",
    "        model=mistral_model,\n",
    "        document={\n",
    "            \"type\": \"document_url\",\n",
    "            \"document_url\": f\"data:application/pdf;base64,{base64_pdf}\"\n",
    "        },\n",
    "        include_image_base64=True\n",
    "        )\n",
    "\n",
    "    # Convert response to JSON format\n",
    "    response_dict = json.loads(pdf_response.model_dump_json())\n",
    "\n",
    "    return response_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELEVE_A: 1.96s\n",
      "ELEVE_B: 1.83s\n",
      "ELEVE_C: 2.53s\n",
      "ELEVE_D: 1.97s\n"
     ]
    }
   ],
   "source": [
    "# Extraire tous les PDFs avec Mistral OCR\n",
    "mistral_results = {}\n",
    "mistral_times = {}\n",
    "\n",
    "for pdf_path in sorted(RAW_DIR.glob(\"*.pdf\")):\n",
    "    eleve_id = pdf_path.stem\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    try:\n",
    "        result = extract_with_mistral_ocr(pdf_path)\n",
    "        mistral_results[eleve_id] = result\n",
    "    except Exception as e:\n",
    "        mistral_results[eleve_id] = f\"ERROR: {e}\"\n",
    "    mistral_times[eleve_id] = time.perf_counter() - start\n",
    "\n",
    "    print(f\"{eleve_id}: {mistral_times[eleve_id]:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"pages\": [\n",
      "        {\n",
      "            \"index\": 0,\n",
      "            \"markdown\": \"# College Test\\nBULLETIN SCOLAIRE\\n\\n\\u00c9l\\u00e8ve : ELEVE_A\\nGenre : Fille\\nAbsences : 4 demi-journ\\u00e9es (justifi\\u00e9es)\\nEngagements : D\\u00e9l\\u00e9gu\\u00e9e titulaire\\n\\n|  Mati\\u00e8re | \\u00c9l\\u00e8ve / Classe | Appr\\u00e9ciation  |\\n| --- | --- | --- |\\n|  Anglais LV1 | 15.21 / 10.83 | Bons r\\u00e9sultats. ELEVE_A fournit un travail r\\u00e9gulier et s\\u00e9rieux \\u00e0 la maison tout comme en classe. L'attitude est toujours positive et constructive. Poursuivez ainsi!  |\\n|  Arts Plastiques | 15.00 / 14.92 | Bon ensemble, bilan satisfaisant, continuez ainsi en restant concentr\\u00e9e.  |\\n|  EPS | 8.00 / 12.79 | Bilan tr\\u00e8s insuffisant, ELEVE_A n'a pas r\\u00e9ussi \\u00e0 s'orienter avec efficacit\\u00e9. Elle a beaucoup march\\u00e9 et s'est dispers\\u00e9e avec son groupe. Son travail a manqu\\u00e9 de s\\u00e9rieux et d'implication dans les d\\u00e9fis \\u00e0 relever en course d'orientation. De si\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(mistral_results[\"ELEVE_A\"], indent=4)[0:1000]) # check the first 1000 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalisation des extractions\n",
    "\n",
    "Convertir les résultats des deux parsers en format uniforme pour comparaison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def extract_key_value(text: str, key: str) -> str | None:\n",
    "    \"\"\"Extrait une valeur pour une clé donnée (pattern générique 'Key : Value').\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    pattern = rf\"{key}\\s*:\\s*([^\\n]+)\"\n",
    "    match = re.search(pattern, text, re.IGNORECASE)\n",
    "    return match.group(1).strip() if match else None\n",
    "\n",
    "\n",
    "def extract_number(text: str) -> float | None:\n",
    "    \"\"\"Extrait le premier nombre d'un texte.\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    match = re.search(r\"(\\d+[,.]?\\d*)\", text)\n",
    "    if match:\n",
    "        return float(match.group(1).replace(\",\", \".\"))\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_note_pair(text: str) -> tuple[float | None, float | None]:\n",
    "    \"\"\"Extrait une paire de notes (élève / classe).\"\"\"\n",
    "    if not text:\n",
    "        return None, None\n",
    "    match = re.search(r\"(\\d+[,.]?\\d*)\\s*[/|]\\s*(\\d+[,.]?\\d*)\", text)\n",
    "    if match:\n",
    "        return float(match.group(1).replace(\",\", \".\")), float(match.group(2).replace(\",\", \".\"))\n",
    "    # Fallback: juste un nombre\n",
    "    num = extract_number(text)\n",
    "    return num, None\n",
    "\n",
    "\n",
    "def parse_raw_tables(tables: list) -> list[dict]:\n",
    "    \"\"\"Parse les tables brutes en liste de matières.\"\"\"\n",
    "    if not tables:\n",
    "        return []\n",
    "\n",
    "    matieres = []\n",
    "    for table in tables:\n",
    "        for row in table:\n",
    "            if not row or len(row) < 2:\n",
    "                continue\n",
    "\n",
    "            # Colonne 0: nom (nettoyer)\n",
    "            nom = \" \".join((row[0] or \"\").split()).strip()\n",
    "            if not nom:\n",
    "                continue\n",
    "\n",
    "            # Colonne 1: notes\n",
    "            note_text = \" \".join((row[1] or \"\").split()).strip()\n",
    "            moy_eleve, moy_classe = extract_note_pair(note_text)\n",
    "\n",
    "            # Ignorer les lignes sans notes (headers)\n",
    "            if moy_eleve is None:\n",
    "                continue\n",
    "\n",
    "            # Dernière colonne non vide: appréciation\n",
    "            appreciation = \"\"\n",
    "            for cell in reversed(row[2:]):\n",
    "                text = \" \".join((cell or \"\").split()).strip()\n",
    "                if text and len(text) > 5:\n",
    "                    appreciation = text\n",
    "                    break\n",
    "\n",
    "            matieres.append({\n",
    "                \"nom\": nom,\n",
    "                \"moy_eleve\": moy_eleve,\n",
    "                \"moy_classe\": moy_classe,\n",
    "                \"appreciation\": appreciation,\n",
    "            })\n",
    "\n",
    "    return matieres\n",
    "\n",
    "\n",
    "def normalize(source: str, data) -> dict | None:\n",
    "    \"\"\"Normalise n'importe quelle source en dict unifié.\"\"\"\n",
    "    if data is None or isinstance(data, str):\n",
    "        return None\n",
    "\n",
    "    if source == \"ground_truth\":\n",
    "        return {\n",
    "            \"eleve_id\": data[\"eleve_id\"],\n",
    "            \"genre\": data[\"genre\"],\n",
    "            \"absences\": data[\"absences_demi_journees\"],\n",
    "            \"engagements\": \", \".join(data.get(\"engagements\", [])),\n",
    "            \"matieres\": [\n",
    "                {\"nom\": m[\"nom\"], \"moy_eleve\": m[\"moyenne_eleve\"],\n",
    "                 \"moy_classe\": m[\"moyenne_classe\"], \"appreciation\": m.get(\"appreciation\", \"\")}\n",
    "                for m in data[\"matieres\"]\n",
    "            ],\n",
    "            \"moyenne_generale\": data.get(\"moyenne_generale\"),\n",
    "        }\n",
    "\n",
    "    if source == \"pdfplumber\":\n",
    "        # Extraction depuis données brutes\n",
    "        raw_text = data.raw_text or \"\"\n",
    "        raw_tables = data.raw_tables or []\n",
    "\n",
    "        return {\n",
    "            \"eleve_id\": extract_key_value(raw_text, r\"[ÉE]l[èe]ve\"),\n",
    "            \"genre\": extract_key_value(raw_text, \"Genre\"),\n",
    "            \"absences\": int(n) if (n := extract_number(extract_key_value(raw_text, \"Absences?\"))) else None,\n",
    "            \"engagements\": extract_key_value(raw_text, \"Engagements?\"),\n",
    "            \"matieres\": parse_raw_tables(raw_tables),\n",
    "            \"moyenne_generale\": extract_number(extract_key_value(raw_text, r\"Moyenne\\s+g[ée]n[ée]rale\")),\n",
    "        }\n",
    "\n",
    "    if source == \"mistral_ocr\":\n",
    "        md = data[\"pages\"][0][\"markdown\"]\n",
    "\n",
    "        # Mêmes patterns génériques\n",
    "        skip_values = {\"Matière\", \":--:\", \"---\"}\n",
    "\n",
    "        return {\n",
    "            \"eleve_id\": extract_key_value(md, r\"[ÉE]l[èe]ve\"),\n",
    "            \"genre\": extract_key_value(md, \"Genre\"),\n",
    "            \"absences\": int(n) if (n := extract_number(extract_key_value(md, \"Absences?\"))) else None,\n",
    "            \"engagements\": extract_key_value(md, \"Engagements?\"),\n",
    "            \"matieres\": [\n",
    "                {\"nom\": r[0].strip(),\n",
    "                 \"moy_eleve\": float(m.group(1)) if (m := re.search(r\"([\\d.]+)\\s*/\", r[1])) else None,\n",
    "                 \"moy_classe\": float(m.group(1)) if (m := re.search(r\"/\\s*([\\d.]+)\", r[1])) else None,\n",
    "                 \"appreciation\": r[2].strip()}\n",
    "                for r in re.findall(r\"\\|\\s*([^|]+)\\s*\\|\\s*([^|]+)\\s*\\|\\s*([^|]*)\\s*\\|\", md)\n",
    "                if r[0].strip() not in skip_values and \"Élève\" not in r[0]\n",
    "            ],\n",
    "            \"moyenne_generale\": extract_number(extract_key_value(md, r\"Moyenne\\s+g[ée]n[ée]rale\")),\n",
    "        }\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# Normaliser tous les résultats\n",
    "normalized = {\"pdfplumber\": {}, \"mistral_ocr\": {}}\n",
    "\n",
    "for eleve_id in gt_by_eleve.keys():\n",
    "    normalized[\"pdfplumber\"][eleve_id] = normalize(\"pdfplumber\", pdfplumber_results.get(eleve_id))\n",
    "    normalized[\"mistral_ocr\"][eleve_id] = normalize(\"mistral_ocr\", mistral_results.get(eleve_id))\n",
    "\n",
    "print(\"Données normalisées pour:\")\n",
    "for source, data in normalized.items():\n",
    "    print(f\"  - {source}: {list(data.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Export des extractions\n",
    "\n",
    "Sauvegarder les extractions en markdown pour faciliter la comparaison visuelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ELEVE_A exporté\n",
      "✓ ELEVE_B exporté\n",
      "✓ ELEVE_C exporté\n",
      "✓ ELEVE_D exporté\n",
      "\n",
      "Fichiers exportés dans: c:\\Users\\Florent\\Documents\\data_science\\chiron\\data\\processed\\benchmark-extraction\n"
     ]
    }
   ],
   "source": [
    "BENCHMARK_DIR = DATA_DIR / \"processed\" / \"benchmark-extraction\"\n",
    "BENCHMARK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def export_to_markdown(data: dict, source: str, output_path: Path):\n",
    "    \"\"\"Exporte les données normalisées en markdown.\"\"\"\n",
    "    lines = [\n",
    "        f\"# {source} - {data.get('eleve_id', 'N/A')}\",\n",
    "        \"\",\n",
    "        f\"**Genre:** {data.get('genre', 'N/A')}\",\n",
    "        f\"**Absences:** {data.get('absences', 'N/A')} demi-journées\",\n",
    "        f\"**Engagements:** {data.get('engagements') or 'Aucun'}\",\n",
    "        \"\",\n",
    "        \"## Matières\",\n",
    "        \"\",\n",
    "        \"| Matière | Moy. Élève | Moy. Classe | Appréciation |\",\n",
    "        \"|---------|------------|-------------|--------------|\",\n",
    "    ]\n",
    "\n",
    "    for m in data.get(\"matieres\", []):\n",
    "        app = (m.get(\"appreciation\") or \"\").replace(\"|\", \"\\\\|\").replace(\"\\n\", \" \")\n",
    "        lines.append(f\"| {m['nom']} | {m['moy_eleve']} | {m['moy_classe']} | {app} |\")\n",
    "\n",
    "    if data.get(\"moyenne_generale\"):\n",
    "        lines.extend([\"\", f\"**Moyenne générale:** {data['moyenne_generale']}/20\"])\n",
    "\n",
    "    output_path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# Exporter pour tous les élèves\n",
    "for eleve_id in gt_by_eleve.keys():\n",
    "    # Ground truth (utilise normalize)\n",
    "    gt_norm = normalize(\"ground_truth\", gt_by_eleve[eleve_id])\n",
    "    export_to_markdown(gt_norm, \"Ground Truth\", BENCHMARK_DIR / f\"{eleve_id}_ground_truth.md\")\n",
    "\n",
    "    # pdfplumber\n",
    "    if normalized[\"pdfplumber\"].get(eleve_id):\n",
    "        export_to_markdown(normalized[\"pdfplumber\"][eleve_id], \"pdfplumber\", BENCHMARK_DIR / f\"{eleve_id}_pdfplumber.md\")\n",
    "\n",
    "    # Mistral OCR\n",
    "    if normalized[\"mistral_ocr\"].get(eleve_id):\n",
    "        export_to_markdown(normalized[\"mistral_ocr\"][eleve_id], \"Mistral OCR\", BENCHMARK_DIR / f\"{eleve_id}_mistral_ocr.md\")\n",
    "\n",
    "    print(f\"✓ {eleve_id} exporté\")\n",
    "\n",
    "print(f\"\\nFichiers exportés dans: {BENCHMARK_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparaison avec le Ground Truth\n",
    "\n",
    "Fonction de comparaison générique pour les deux sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comparaisons: 96 lignes\n"
     ]
    }
   ],
   "source": [
    "def compare_with_gt(extracted: dict, gt: dict, source: str) -> pd.DataFrame:\n",
    "    \"\"\"Compare les données extraites avec le ground truth.\n",
    "\n",
    "    Args:\n",
    "        extracted: Données normalisées extraites\n",
    "        gt: Ground truth\n",
    "        source: Nom de la source (pdfplumber, mistral_ocr)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame avec les résultats de comparaison\n",
    "    \"\"\"\n",
    "    if not extracted:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    comparisons = []\n",
    "    gt_matieres = {m[\"nom\"]: m for m in gt[\"matieres\"]}\n",
    "\n",
    "    for m in extracted[\"matieres\"]:\n",
    "        nom = m[\"nom\"]\n",
    "        gt_m = gt_matieres.get(nom, {})\n",
    "\n",
    "        # Comparer moyennes\n",
    "        moy_eleve_ok = m[\"moy_eleve\"] == gt_m.get(\"moyenne_eleve\")\n",
    "        moy_classe_ok = m[\"moy_classe\"] == gt_m.get(\"moyenne_classe\")\n",
    "\n",
    "        # Comparer appréciations (normaliser les espaces)\n",
    "        app_ext = \" \".join((m.get(\"appreciation\") or \"\").split())\n",
    "        app_gt = \" \".join(gt_m.get(\"appreciation\", \"\").split())\n",
    "        app_ok = app_ext == app_gt\n",
    "\n",
    "        comparisons.append({\n",
    "            \"source\": source,\n",
    "            \"matiere\": nom,\n",
    "            \"moy_eleve_ext\": m[\"moy_eleve\"],\n",
    "            \"moy_eleve_gt\": gt_m.get(\"moyenne_eleve\"),\n",
    "            \"moy_eleve_ok\": \"✅\" if moy_eleve_ok else \"❌\",\n",
    "            \"moy_classe_ext\": m[\"moy_classe\"],\n",
    "            \"moy_classe_gt\": gt_m.get(\"moyenne_classe\"),\n",
    "            \"moy_classe_ok\": \"✅\" if moy_classe_ok else \"❌\",\n",
    "            \"app_ok\": \"✅\" if app_ok else \"❌\",\n",
    "            \"app_len_ext\": len(app_ext),\n",
    "            \"app_len_gt\": len(app_gt),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(comparisons)\n",
    "\n",
    "\n",
    "# Comparer toutes les sources pour tous les élèves\n",
    "all_comparisons = []\n",
    "\n",
    "for source_name, source_data in normalized.items():\n",
    "    for eleve_id, extracted in source_data.items():\n",
    "        df = compare_with_gt(extracted, gt_by_eleve[eleve_id], source_name)\n",
    "        if not df.empty:\n",
    "            df[\"eleve_id\"] = eleve_id\n",
    "            all_comparisons.append(df)\n",
    "\n",
    "df_comparison = pd.concat(all_comparisons, ignore_index=True)\n",
    "print(f\"Total comparaisons: {len(df_comparison)} lignes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STATISTIQUES PAR SOURCE\n",
      "============================================================\n",
      "     source  total_matieres  moy_eleve_%  moy_classe_%  app_%\n",
      " pdfplumber              50        100.0         100.0   92.0\n",
      "mistral_ocr              46        100.0         100.0  100.0\n"
     ]
    }
   ],
   "source": [
    "# Statistiques par source\n",
    "stats_by_source = []\n",
    "\n",
    "for source in df_comparison[\"source\"].unique():\n",
    "    df_src = df_comparison[df_comparison[\"source\"] == source]\n",
    "    total = len(df_src)\n",
    "\n",
    "    stats_by_source.append({\n",
    "        \"source\": source,\n",
    "        \"total_matieres\": total,\n",
    "        \"moy_eleve_ok\": (df_src[\"moy_eleve_ok\"] == \"✅\").sum(),\n",
    "        \"moy_classe_ok\": (df_src[\"moy_classe_ok\"] == \"✅\").sum(),\n",
    "        \"app_ok\": (df_src[\"app_ok\"] == \"✅\").sum(),\n",
    "    })\n",
    "\n",
    "df_stats = pd.DataFrame(stats_by_source)\n",
    "df_stats[\"moy_eleve_%\"] = (df_stats[\"moy_eleve_ok\"] / df_stats[\"total_matieres\"] * 100).round(1)\n",
    "df_stats[\"moy_classe_%\"] = (df_stats[\"moy_classe_ok\"] / df_stats[\"total_matieres\"] * 100).round(1)\n",
    "df_stats[\"app_%\"] = (df_stats[\"app_ok\"] / df_stats[\"total_matieres\"] * 100).round(1)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STATISTIQUES PAR SOURCE\")\n",
    "print(\"=\" * 60)\n",
    "print(df_stats[[\"source\", \"total_matieres\", \"moy_eleve_%\", \"moy_classe_%\", \"app_%\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appréciations différentes (4):\n",
      "\n",
      "    source eleve_id matiere  app_len_ext  app_len_gt\n",
      "pdfplumber  ELEVE_A Matière           12           0\n",
      "pdfplumber  ELEVE_B Matière           12           0\n",
      "pdfplumber  ELEVE_C Matière           12           0\n",
      "pdfplumber  ELEVE_D Matière           12           0\n"
     ]
    }
   ],
   "source": [
    "# Détail des appréciations qui ne matchent pas\n",
    "df_errors = df_comparison[df_comparison[\"app_ok\"] == \"❌\"][\n",
    "    [\"source\", \"eleve_id\", \"matiere\", \"app_len_ext\", \"app_len_gt\"]\n",
    "]\n",
    "print(f\"Appréciations différentes ({len(df_errors)}):\\n\")\n",
    "if not df_errors.empty:\n",
    "    print(df_errors.to_string(index=False))\n",
    "else:\n",
    "    print(\"Aucune erreur d'appréciation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Résumé comparatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution (secondes):\n",
      "eleve_id  pdfplumber_time  mistral_ocr_time\n",
      " ELEVE_A         0.064692          1.960043\n",
      " ELEVE_B         0.063142          1.831447\n",
      " ELEVE_C         0.055726          2.527051\n",
      " ELEVE_D         0.065341          1.968609\n",
      "\n",
      "Moyenne pdfplumber: 0.06s\n",
      "Moyenne Mistral OCR: 2.07s\n",
      "Ratio: 33x plus lent\n"
     ]
    }
   ],
   "source": [
    "# Temps d'exécution\n",
    "df_times = pd.DataFrame([\n",
    "    {\n",
    "        \"eleve_id\": eleve_id,\n",
    "        \"pdfplumber_time\": pdfplumber_times.get(eleve_id),\n",
    "        \"mistral_ocr_time\": mistral_times.get(eleve_id),\n",
    "    }\n",
    "    for eleve_id in gt_by_eleve.keys()\n",
    "])\n",
    "\n",
    "print(\"Temps d'exécution (secondes):\")\n",
    "print(df_times.to_string(index=False))\n",
    "print(f\"\\nMoyenne pdfplumber: {df_times['pdfplumber_time'].mean():.2f}s\")\n",
    "print(f\"Moyenne Mistral OCR: {df_times['mistral_ocr_time'].mean():.2f}s\")\n",
    "print(f\"Ratio: {df_times['mistral_ocr_time'].mean() / df_times['pdfplumber_time'].mean():.0f}x plus lent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RÉSUMÉ BENCHMARK\n",
      "======================================================================\n",
      "                 Métrique pdfplumber mistral_ocr\n",
      "          Temps moyen (s)       0.06        2.07\n",
      " Moyennes élève correctes     100.0%      100.0%\n",
      "Moyennes classe correctes     100.0%      100.0%\n",
      " Appréciations identiques      92.0%      100.0%\n",
      "\n",
      "======================================================================\n",
      "CONCLUSION\n",
      "======================================================================\n",
      "\n",
      "## Performance\n",
      "- pdfplumber est ~33x plus rapide que Mistral OCR\n",
      "- Les deux atteignent 100% de précision sur ce format standardisé\n",
      "\n",
      "## Différences clés\n",
      "\n",
      "**pdfplumber** (extraction mécanique) :\n",
      "- Retourne les données brutes (tables + texte) sans interprétation\n",
      "- Problème : headers multi-lignes (\"Élève /\\nClasse\") nécessitent post-traitement\n",
      "- Nécessite : filtrage des headers, parsing des notes, normalisation\n",
      "- Adapté pour : documents avec format connu et stable (ex: PRONOTE)\n",
      "\n",
      "**Mistral OCR** (extraction intelligente) :\n",
      "- Interprète et reconstruit la structure sémantique (markdown propre)\n",
      "- Headers correctement fusionnés dès l'extraction (\"Élève / Classe\")\n",
      "- Post-traitement minimal : simple parsing du markdown standard\n",
      "- Adapté pour : documents variés ou formats inconnus\n",
      "\n",
      "## Recommandation\n",
      "- Format unique (PRONOTE) → pdfplumber + normalize() (rapide, gratuit)\n",
      "- Formats variés (multi-établissements) → Mistral OCR (plus robuste)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tableau récapitulatif final\n",
    "print(\"=\" * 70)\n",
    "print(\"RÉSUMÉ BENCHMARK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"Métrique\": [\n",
    "        \"Temps moyen (s)\",\n",
    "        \"Moyennes élève correctes\",\n",
    "        \"Moyennes classe correctes\",\n",
    "        \"Appréciations identiques\",\n",
    "    ],\n",
    "    \"pdfplumber\": [\n",
    "        f\"{df_times['pdfplumber_time'].mean():.2f}\",\n",
    "        f\"{df_stats[df_stats['source'] == 'pdfplumber']['moy_eleve_%'].values[0]}%\",\n",
    "        f\"{df_stats[df_stats['source'] == 'pdfplumber']['moy_classe_%'].values[0]}%\",\n",
    "        f\"{df_stats[df_stats['source'] == 'pdfplumber']['app_%'].values[0]}%\",\n",
    "    ],\n",
    "    \"mistral_ocr\": [\n",
    "        f\"{df_times['mistral_ocr_time'].mean():.2f}\",\n",
    "        f\"{df_stats[df_stats['source'] == 'mistral_ocr']['moy_eleve_%'].values[0]}%\",\n",
    "        f\"{df_stats[df_stats['source'] == 'mistral_ocr']['moy_classe_%'].values[0]}%\",\n",
    "        f\"{df_stats[df_stats['source'] == 'mistral_ocr']['app_%'].values[0]}%\",\n",
    "    ],\n",
    "})\n",
    "\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "# Calcul dynamique du ratio\n",
    "ratio = df_times['mistral_ocr_time'].mean() / df_times['pdfplumber_time'].mean()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\"\"\n",
    "## Performance\n",
    "- pdfplumber est ~{ratio:.0f}x plus rapide que Mistral OCR\n",
    "- Les deux atteignent 100% de précision sur ce format standardisé\n",
    "\n",
    "## Différences clés\n",
    "\n",
    "**pdfplumber** (extraction mécanique) :\n",
    "- Retourne les données brutes (tables + texte) sans interprétation\n",
    "- Problème : headers multi-lignes (\"Élève /\\\\nClasse\") nécessitent post-traitement\n",
    "- Nécessite : filtrage des headers, parsing des notes, normalisation\n",
    "- Adapté pour : documents avec format connu et stable (ex: PRONOTE)\n",
    "\n",
    "**Mistral OCR** (extraction intelligente) :\n",
    "- Interprète et reconstruit la structure sémantique (markdown propre)\n",
    "- Headers correctement fusionnés dès l'extraction (\"Élève / Classe\")\n",
    "- Post-traitement minimal : simple parsing du markdown standard\n",
    "- Adapté pour : documents variés ou formats inconnus\n",
    "\n",
    "## Recommandation\n",
    "- Format unique (PRONOTE) → pdfplumber + normalize() (rapide, gratuit)\n",
    "- Formats variés (multi-établissements) → Mistral OCR (plus robuste)\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chiron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
