{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Parser Benchmark\n",
    "\n",
    "Comparaison des méthodes d'extraction PDF :\n",
    "- **pdfplumber** : Parser mécanique (extraction de tableaux)\n",
    "- **Mistral OCR** : Vision model cloud (mistral-ocr-2503)\n",
    "- **Docling** : Vision model local open source (IBM)\n",
    "\n",
    "## Objectif\n",
    "Mesurer la précision d'extraction des bulletins scolaires vs ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\Florent\\Documents\\data_science\\chiron\n",
      "PDFs: [WindowsPath('c:/Users/Florent/Documents/data_science/chiron/data/raw/ELEVE_A.pdf'), WindowsPath('c:/Users/Florent/Documents/data_science/chiron/data/raw/ELEVE_B.pdf'), WindowsPath('c:/Users/Florent/Documents/data_science/chiron/data/raw/ELEVE_C.pdf'), WindowsPath('c:/Users/Florent/Documents/data_science/chiron/data/raw/ELEVE_D.pdf')]\n"
     ]
    }
   ],
   "source": [
    "# ruff: noqa: E402\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Auto-détecter project_root\n",
    "current = Path.cwd()\n",
    "while current != current.parent:\n",
    "    if (current / \"pyproject.toml\").exists():\n",
    "        project_root = current\n",
    "        break\n",
    "    current = current.parent\n",
    "\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(project_root / \".env\")\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = project_root / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "GROUND_TRUTH_PATH = DATA_DIR / \"ground_truth\" / \"chiron_ground_truth.json\"\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PDFs: {list(RAW_DIR.glob('*.pdf'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: ['ELEVE_A', 'ELEVE_B', 'ELEVE_C', 'ELEVE_D']\n"
     ]
    }
   ],
   "source": [
    "# Charger ground truth\n",
    "with open(GROUND_TRUTH_PATH, encoding=\"utf-8\") as f:\n",
    "    ground_truth = json.load(f)\n",
    "\n",
    "# Index par eleve_id pour comparaison facile\n",
    "gt_by_eleve = {e[\"eleve_id\"]: e for e in ground_truth[\"eleves\"]}\n",
    "print(f\"Ground truth: {list(gt_by_eleve.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parser pdfplumber (actuel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.document import ParserType, get_parser\n",
    "\n",
    "parser = get_parser(ParserType.PDFPLUMBER)\n",
    "\n",
    "pdfplumber_results = {}\n",
    "pdfplumber_times = {}\n",
    "\n",
    "for pdf_path in sorted(RAW_DIR.glob(\"*.pdf\")):\n",
    "    eleve_id = pdf_path.stem  # ELEVE_A, ELEVE_B, etc.\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    try:\n",
    "        eleves = parser.parse(pdf_path)\n",
    "        pdfplumber_results[eleve_id] = eleves[0] if eleves else None\n",
    "    except Exception as e:\n",
    "        pdfplumber_results[eleve_id] = f\"ERROR: {e}\"\n",
    "    pdfplumber_times[eleve_id] = time.perf_counter() - start\n",
    "\n",
    "    print(f\"{eleve_id}: {pdfplumber_times[eleve_id]:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nom: None\n",
      "Prénom: None\n",
      "Classe: None\n",
      "Matières extraites: 0\n"
     ]
    }
   ],
   "source": [
    "# Afficher un exemple de résultat pdfplumber\n",
    "exemple = pdfplumber_results.get(\"ELEVE_A\")\n",
    "if exemple and not isinstance(exemple, str):\n",
    "    print(f\"Nom: {exemple.nom}\")\n",
    "    print(f\"Prénom: {exemple.prenom}\")\n",
    "    print(f\"Classe: {exemple.classe}\")\n",
    "    print(f\"Matières extraites: {len(exemple.matieres)}\")\n",
    "    for m in exemple.matieres[:]:\n",
    "        print(f\"  - {m.nom}: {m.moyenne_eleve}\")\n",
    "else:\n",
    "    print(f\"Erreur ou pas de résultat: {exemple}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mistral OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client Mistral initialisé\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "from mistralai import Mistral\n",
    "\n",
    "# Init client\n",
    "mistral_api_key = os.getenv(\"MISTRAL_OCR_API_KEY\")\n",
    "if not mistral_api_key:\n",
    "    raise ValueError(\"MISTRAL_OCR_API_KEY non configurée dans .env\")\n",
    "\n",
    "client = Mistral(api_key=mistral_api_key)\n",
    "mistral_model = \"mistral-ocr-latest\"\n",
    "print(\"Client Mistral initialisé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_pdf(pdf_path):\n",
    "    \"\"\"Encode the pdf to base64.\"\"\"\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as pdf_file:\n",
    "            return base64.b64encode(pdf_file.read()).decode('utf-8')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {pdf_path} was not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_with_mistral_ocr(pdf_path: Path) -> dict:\n",
    "    \"\"\"Extrait le contenu d'un PDF avec Mistral OCR.\"\"\"\n",
    "    # Getting the base64 string\n",
    "    base64_pdf = encode_pdf(pdf_path)\n",
    "\n",
    "    # Call the OCR API\n",
    "    pdf_response = client.ocr.process(\n",
    "        model=mistral_model,\n",
    "        document={\n",
    "            \"type\": \"document_url\",\n",
    "            \"document_url\": f\"data:application/pdf;base64,{base64_pdf}\"\n",
    "        },\n",
    "        include_image_base64=True\n",
    "        )\n",
    "\n",
    "    # Convert response to JSON format\n",
    "    response_dict = json.loads(pdf_response.model_dump_json())\n",
    "\n",
    "    return response_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELEVE_A: 1.96s\n",
      "ELEVE_B: 2.08s\n",
      "ELEVE_C: 1.72s\n",
      "ELEVE_D: 1.90s\n"
     ]
    }
   ],
   "source": [
    "# Extraire tous les PDFs avec Mistral OCR\n",
    "mistral_results = {}\n",
    "mistral_times = {}\n",
    "\n",
    "for pdf_path in sorted(RAW_DIR.glob(\"*.pdf\")):\n",
    "    eleve_id = pdf_path.stem\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    try:\n",
    "        result = extract_with_mistral_ocr(pdf_path)\n",
    "        mistral_results[eleve_id] = result\n",
    "    except Exception as e:\n",
    "        mistral_results[eleve_id] = f\"ERROR: {e}\"\n",
    "    mistral_times[eleve_id] = time.perf_counter() - start\n",
    "\n",
    "    print(f\"{eleve_id}: {mistral_times[eleve_id]:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"pages\": [\n",
      "        {\n",
      "            \"index\": 0,\n",
      "            \"markdown\": \"# College Test\\nBULLETIN SCOLAIRE\\n\\n\\u00c9l\\u00e8ve : ELEVE_A\\nGenre : Fille\\nAbsences : 4 demi-journ\\u00e9es (justifi\\u00e9es)\\nEngagements : D\\u00e9l\\u00e9gu\\u00e9e titulaire\\n\\n|  Mati\\u00e8re | \\u00c9l\\u00e8ve / Classe | Appr\\u00e9ciation  |\\n| --- | --- | --- |\\n|  Anglais LV1 | 15.21 / 10.83 | Bons r\\u00e9sultats. ELEVE_A fournit un travail r\\u00e9gulier et s\\u00e9rieux \\u00e0 la maison tout comme en classe. L'attitude est toujours positive et constructive. Poursuivez ainsi!  |\\n|  Arts Plastiques | 15.00 / 14.92 | Bon ensemble, bilan satisfaisant, continuez ainsi en restant concentr\\u00e9e.  |\\n|  EPS | 8.00 / 12.79 | Bilan tr\\u00e8s insuffisant, ELEVE_A n'a pas r\\u00e9ussi \\u00e0 s'orienter avec efficacit\\u00e9. Elle a beaucoup march\\u00e9 et s'est dispers\\u00e9e avec son groupe. Son travail a manqu\\u00e9 de s\\u00e9rieux et d'implication dans les d\\u00e9fis \\u00e0 relever en course d'orientation. De si\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(mistral_results[\"ELEVE_A\"], indent=4)[0:1000]) # check the first 1000 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Docling (IBM - Open Source Local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Florent\\Documents\\data_science\\chiron\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docling converter initialisé\n"
     ]
    }
   ],
   "source": [
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions, TableFormerMode\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "pipeline_options = PdfPipelineOptions(do_table_structure=True)\n",
    "pipeline_options.table_structure_options.mode = TableFormerMode.ACCURATE  # use more accurate TableFormer model\n",
    "\n",
    "# Init converter (première exécution télécharge les modèles)\n",
    "docling_converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "    }\n",
    ")\n",
    "print(\"Docling converter initialisé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2026-01-22 10:38:33,377 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-22 10:38:33,389 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-22 10:38:33,402 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\Florent\\Documents\\data_science\\chiron\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-22 10:38:33,404 [RapidOCR] main.py:50: Using C:\\Users\\Florent\\Documents\\data_science\\chiron\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-22 10:38:33,848 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-22 10:38:33,849 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-22 10:38:33,851 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\Florent\\Documents\\data_science\\chiron\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-22 10:38:33,852 [RapidOCR] main.py:50: Using C:\\Users\\Florent\\Documents\\data_science\\chiron\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-22 10:38:33,945 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-22 10:38:33,946 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-22 10:38:33,969 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\Florent\\Documents\\data_science\\chiron\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-22 10:38:33,970 [RapidOCR] main.py:50: Using C:\\Users\\Florent\\Documents\\data_science\\chiron\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELEVE_A: 22.32s\n",
      "ELEVE_B: 4.44s\n",
      "ELEVE_C: 4.60s\n",
      "ELEVE_D: 4.85s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extraire tous les PDFs avec Docling\n",
    "docling_results = {}\n",
    "docling_times = {}\n",
    "\n",
    "for pdf_path in sorted(RAW_DIR.glob(\"*.pdf\")):\n",
    "    eleve_id = pdf_path.stem\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    try:\n",
    "        result = docling_converter.convert(str(pdf_path))\n",
    "        # Export direct en markdown (comme Mistral OCR)\n",
    "        docling_results[eleve_id] = result.document.export_to_markdown()\n",
    "    except Exception as e:\n",
    "        docling_results[eleve_id] = f\"ERROR: {e}\"\n",
    "    docling_times[eleve_id] = time.perf_counter() - start\n",
    "\n",
    "    print(f\"{eleve_id}: {docling_times[eleve_id]:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## College Test BULLETIN SCOLAIRE\n",
      "\n",
      "Élève\n",
      "\n",
      ": ELEVE\\_A\n",
      "\n",
      "Genre\n",
      "\n",
      ": Fille\n",
      "\n",
      "Absences\n",
      "\n",
      ": 4 demi-journées (justifiées)\n",
      "\n",
      "Engagements\n",
      "\n",
      ": Déléguée titulaire\n",
      "\n",
      "| Matière                 | Élève / Classe   | Appréciation                                                                                                                                                                                                                                                               |\n",
      "|-------------------------|------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| Anglais LV1             | 15.21 / 10.83    | Bons résultats. ELEVE_A fournit un travail régulier et sérieux à la maison comme en classe. L'attitude est toujours positive et constructive. ainsi!                                                                                                                       |\n",
      "| Arts Plastiques         | 15.00 / 14.92    | Bon ensemble, bilan satisfaisant, continuez ainsi en restant concentrée.                                                                                                                                                                                                   |\n",
      "| EPS                     | 8.00 / 12.79     | Bilan très insuffisant, ELEVE_A n'a pas réussi à s'orienter avec beaucoup marché et s'est dispersée avec son groupe. Son travail a manqué sérieux et d'implication dans les défis à relever en course d'orientation. sincères efforts sont attendus au prochain trimestre. |\n",
      "| Éducation Musicale      | 13.00 / 9.53     | C'est très bien quand vous voulez. Votre investissement doit être                                                                                                                                                                                                          |\n",
      "| Espagnol LV2            | 13.83 / 10.76    |                                                                                                                                                                                                                                                                            |\n",
      "| Français                | 13.21 / 10.12    | Assez bons résultats à l'écrit. La participation orale est satisfaisante. progresser encore et consolider les acquis, ELEVE_A doit veiller à rester concentrée en classe.                                                                                                  |\n",
      "| Histoire-Géographie-EMC | 14.66 / 10.77    | Bon trimestre ! ELEVE_A est une élève impliquée et sérieuse, en témoigne résultats. Je l'encourage à conserver cette motivation pour le second                                                                                                                             |\n",
      "| Latin et Grec           | 18.21 / 17.77    | Très bon travail. Élève volontaire et investie.                                                                                                                                                                                                                            |\n",
      "| Mathématiques           | 16.07 / 11.75    | Très bon trimestre. ELEVE_A a de bons acquis, elle est volontaire, sa participation est active et son travail appliqué. Je l'encourage à cette dynamique, en veillant à ne pas se disperser.                                                                               |\n",
      "| Physique-Chimie         | 15.71 / 12.49    | C'est un très bon trimestre. La participation de ELEVE_A en classe est précieuse.                                                                                                                                                                                          |\n",
      "| SVT                     | 10.86 / 8.68     | Bilan un peu juste de ELEVE_A. Le travail personnel est encore perfectible comme la participation en classe qui est encore trop timide.                                                                                                                                    |\n",
      "| Technologie             | 15.80 / 13.73    | Très bon trimestre, c'est très bien, continuez ainsi.                                                                                                                                                                                                                      |\n",
      "\n",
      "Moyenne générale\n",
      "\n",
      ": 14.13/20\n"
     ]
    }
   ],
   "source": [
    "print(docling_results[\"ELEVE_A\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Normalisation des extractions\n",
    "\n",
    "Convertir les résultats des trois parsers en format uniforme pour comparaison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données normalisées pour:\n",
      "  - pdfplumber: ['ELEVE_A', 'ELEVE_B', 'ELEVE_C', 'ELEVE_D']\n",
      "  - mistral_ocr: ['ELEVE_A', 'ELEVE_B', 'ELEVE_C', 'ELEVE_D']\n",
      "  - docling: ['ELEVE_A', 'ELEVE_B', 'ELEVE_C', 'ELEVE_D']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def extract_key_value(text: str, key: str) -> str | None:\n",
    "    \"\"\"Extrait une valeur pour une clé donnée.\n",
    "\n",
    "    Gère deux formats :\n",
    "    - 'Key : Value' (même ligne)\n",
    "    - 'Key\\\\n: Value' (ligne séparée, format Docling)\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    # Format standard: Key : Value\n",
    "    pattern = rf\"{key}\\s*:\\s*([^\\n]+)\"\n",
    "    match = re.search(pattern, text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    # Format Docling: Key\\n: Value\n",
    "    pattern_docling = rf\"{key}\\s*\\n\\s*:\\s*([^\\n]+)\"\n",
    "    match = re.search(pattern_docling, text, re.IGNORECASE)\n",
    "    return match.group(1).strip() if match else None\n",
    "\n",
    "\n",
    "def extract_number(text: str) -> float | None:\n",
    "    \"\"\"Extrait le premier nombre d'un texte.\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    match = re.search(r\"(\\d+[,.]?\\d*)\", text)\n",
    "    if match:\n",
    "        return float(match.group(1).replace(\",\", \".\"))\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_note_pair(text: str) -> tuple[float | None, float | None]:\n",
    "    \"\"\"Extrait une paire de notes (élève / classe).\"\"\"\n",
    "    if not text:\n",
    "        return None, None\n",
    "    match = re.search(r\"(\\d+[,.]?\\d*)\\s*[/|]\\s*(\\d+[,.]?\\d*)\", text)\n",
    "    if match:\n",
    "        return float(match.group(1).replace(\",\", \".\")), float(match.group(2).replace(\",\", \".\"))\n",
    "    # Fallback: juste un nombre\n",
    "    num = extract_number(text)\n",
    "    return num, None\n",
    "\n",
    "\n",
    "def parse_raw_tables(tables: list) -> list[dict]:\n",
    "    \"\"\"Parse les tables brutes en liste de matières.\"\"\"\n",
    "    if not tables:\n",
    "        return []\n",
    "\n",
    "    matieres = []\n",
    "    for table in tables:\n",
    "        for row in table:\n",
    "            if not row or len(row) < 2:\n",
    "                continue\n",
    "\n",
    "            # Colonne 0: nom (nettoyer)\n",
    "            nom = \" \".join((row[0] or \"\").split()).strip()\n",
    "            if not nom:\n",
    "                continue\n",
    "\n",
    "            # Colonne 1: notes\n",
    "            note_text = \" \".join((row[1] or \"\").split()).strip()\n",
    "            moy_eleve, moy_classe = extract_note_pair(note_text)\n",
    "\n",
    "            # Ignorer les lignes sans notes (headers)\n",
    "            if moy_eleve is None:\n",
    "                continue\n",
    "\n",
    "            # Dernière colonne non vide: appréciation\n",
    "            appreciation = \"\"\n",
    "            for cell in reversed(row[2:]):\n",
    "                text = \" \".join((cell or \"\").split()).strip()\n",
    "                if text and len(text) > 5:\n",
    "                    appreciation = text\n",
    "                    break\n",
    "\n",
    "            matieres.append({\n",
    "                \"nom\": nom,\n",
    "                \"moy_eleve\": moy_eleve,\n",
    "                \"moy_classe\": moy_classe,\n",
    "                \"appreciation\": appreciation,\n",
    "            })\n",
    "\n",
    "    return matieres\n",
    "\n",
    "\n",
    "def parse_markdown_table(md: str) -> list[dict]:\n",
    "    \"\"\"Parse un tableau markdown en liste de matières.\"\"\"\n",
    "    skip_values = {\"Matière\", \":--:\", \"---\", \"\"}\n",
    "    matieres = []\n",
    "\n",
    "    for r in re.findall(r\"\\|\\s*([^|]+)\\s*\\|\\s*([^|]+)\\s*\\|\\s*([^|]*)\\s*\\|\", md):\n",
    "        nom = r[0].strip()\n",
    "        if nom in skip_values or \"Élève\" in nom or \"lève\" in nom:\n",
    "            continue\n",
    "\n",
    "        moy_eleve = float(m.group(1)) if (m := re.search(r\"([\\d.]+)\\s*/\", r[1])) else None\n",
    "        moy_classe = float(m.group(1)) if (m := re.search(r\"/\\s*([\\d.]+)\", r[1])) else None\n",
    "\n",
    "        # Ignorer les lignes sans notes\n",
    "        if moy_eleve is None:\n",
    "            continue\n",
    "\n",
    "        matieres.append({\n",
    "            \"nom\": nom,\n",
    "            \"moy_eleve\": moy_eleve,\n",
    "            \"moy_classe\": moy_classe,\n",
    "            \"appreciation\": r[2].strip() if len(r) > 2 else \"\",\n",
    "        })\n",
    "\n",
    "    return matieres\n",
    "\n",
    "\n",
    "def normalize(source: str, data) -> dict | None:\n",
    "    \"\"\"Normalise n'importe quelle source en dict unifié.\"\"\"\n",
    "    if data is None or isinstance(data, str) and data.startswith(\"ERROR\"):\n",
    "        return None\n",
    "\n",
    "    if source == \"ground_truth\":\n",
    "        return {\n",
    "            \"eleve_id\": data[\"eleve_id\"],\n",
    "            \"genre\": data[\"genre\"],\n",
    "            \"absences\": data[\"absences_demi_journees\"],\n",
    "            \"engagements\": \", \".join(data.get(\"engagements\", [])),\n",
    "            \"matieres\": [\n",
    "                {\"nom\": m[\"nom\"], \"moy_eleve\": m[\"moyenne_eleve\"],\n",
    "                 \"moy_classe\": m[\"moyenne_classe\"], \"appreciation\": m.get(\"appreciation\", \"\")}\n",
    "                for m in data[\"matieres\"]\n",
    "            ],\n",
    "            \"moyenne_generale\": data.get(\"moyenne_generale\"),\n",
    "        }\n",
    "\n",
    "    if source == \"pdfplumber\":\n",
    "        raw_text = data.raw_text or \"\"\n",
    "        raw_tables = data.raw_tables or []\n",
    "\n",
    "        return {\n",
    "            \"eleve_id\": extract_key_value(raw_text, r\"[ÉE]l[èe]ve\"),\n",
    "            \"genre\": extract_key_value(raw_text, \"Genre\"),\n",
    "            \"absences\": int(n) if (n := extract_number(extract_key_value(raw_text, \"Absences?\"))) else None,\n",
    "            \"engagements\": extract_key_value(raw_text, \"Engagements?\"),\n",
    "            \"matieres\": parse_raw_tables(raw_tables),\n",
    "            \"moyenne_generale\": extract_number(extract_key_value(raw_text, r\"Moyenne\\s+g[ée]n[ée]rale\")),\n",
    "        }\n",
    "\n",
    "    if source == \"mistral_ocr\":\n",
    "        md = data[\"pages\"][0][\"markdown\"]\n",
    "        return {\n",
    "            \"eleve_id\": extract_key_value(md, r\"[ÉE]l[èe]ve\"),\n",
    "            \"genre\": extract_key_value(md, \"Genre\"),\n",
    "            \"absences\": int(n) if (n := extract_number(extract_key_value(md, \"Absences?\"))) else None,\n",
    "            \"engagements\": extract_key_value(md, \"Engagements?\"),\n",
    "            \"matieres\": parse_markdown_table(md),\n",
    "            \"moyenne_generale\": extract_number(extract_key_value(md, r\"Moyenne\\s+g[ée]n[ée]rale\")),\n",
    "        }\n",
    "\n",
    "    if source == \"docling\":\n",
    "        # Docling retourne directement du markdown (string)\n",
    "        md = data if isinstance(data, str) else \"\"\n",
    "        return {\n",
    "            \"eleve_id\": extract_key_value(md, r\"[ÉE]l[èe]ve\"),\n",
    "            \"genre\": extract_key_value(md, \"Genre\"),\n",
    "            \"absences\": int(n) if (n := extract_number(extract_key_value(md, \"Absences?\"))) else None,\n",
    "            \"engagements\": extract_key_value(md, \"Engagements?\"),\n",
    "            \"matieres\": parse_markdown_table(md),\n",
    "            \"moyenne_generale\": extract_number(extract_key_value(md, r\"Moyenne\\s+g[ée]n[ée]rale\")),\n",
    "        }\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# Normaliser tous les résultats\n",
    "normalized = {\"pdfplumber\": {}, \"mistral_ocr\": {}, \"docling\": {}}\n",
    "\n",
    "for eleve_id in gt_by_eleve.keys():\n",
    "    normalized[\"pdfplumber\"][eleve_id] = normalize(\"pdfplumber\", pdfplumber_results.get(eleve_id))\n",
    "    normalized[\"mistral_ocr\"][eleve_id] = normalize(\"mistral_ocr\", mistral_results.get(eleve_id))\n",
    "    normalized[\"docling\"][eleve_id] = normalize(\"docling\", docling_results.get(eleve_id))\n",
    "\n",
    "print(\"Données normalisées pour:\")\n",
    "for source, data in normalized.items():\n",
    "    print(f\"  - {source}: {list(data.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export des extractions\n",
    "\n",
    "Sauvegarder les extractions en markdown pour faciliter la comparaison visuelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ELEVE_A exporté\n",
      "✓ ELEVE_B exporté\n",
      "✓ ELEVE_C exporté\n",
      "✓ ELEVE_D exporté\n",
      "\n",
      "Fichiers exportés dans: c:\\Users\\Florent\\Documents\\data_science\\chiron\\data\\processed\\benchmark-extraction\n"
     ]
    }
   ],
   "source": [
    "BENCHMARK_DIR = DATA_DIR / \"processed\" / \"benchmark-extraction\"\n",
    "BENCHMARK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def export_to_markdown(data: dict, source: str, output_path: Path):\n",
    "    \"\"\"Exporte les données normalisées en markdown.\"\"\"\n",
    "    lines = [\n",
    "        f\"# {source} - {data.get('eleve_id', 'N/A')}\",\n",
    "        \"\",\n",
    "        f\"**Genre:** {data.get('genre', 'N/A')}\",\n",
    "        f\"**Absences:** {data.get('absences', 'N/A')} demi-journées\",\n",
    "        f\"**Engagements:** {data.get('engagements') or 'Aucun'}\",\n",
    "        \"\",\n",
    "        \"## Matières\",\n",
    "        \"\",\n",
    "        \"| Matière | Moy. Élève | Moy. Classe | Appréciation |\",\n",
    "        \"|---------|------------|-------------|--------------|\",\n",
    "    ]\n",
    "\n",
    "    for m in data.get(\"matieres\", []):\n",
    "        app = (m.get(\"appreciation\") or \"\").replace(\"|\", \"\\\\|\").replace(\"\\n\", \" \")\n",
    "        lines.append(f\"| {m['nom']} | {m['moy_eleve']} | {m['moy_classe']} | {app} |\")\n",
    "\n",
    "    if data.get(\"moyenne_generale\"):\n",
    "        lines.extend([\"\", f\"**Moyenne générale:** {data['moyenne_generale']}/20\"])\n",
    "\n",
    "    output_path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# Exporter pour tous les élèves\n",
    "for eleve_id in gt_by_eleve.keys():\n",
    "    # Ground truth\n",
    "    gt_norm = normalize(\"ground_truth\", gt_by_eleve[eleve_id])\n",
    "    export_to_markdown(gt_norm, \"Ground Truth\", BENCHMARK_DIR / f\"{eleve_id}_ground_truth.md\")\n",
    "\n",
    "    # pdfplumber\n",
    "    if normalized[\"pdfplumber\"].get(eleve_id):\n",
    "        export_to_markdown(normalized[\"pdfplumber\"][eleve_id], \"pdfplumber\", BENCHMARK_DIR / f\"{eleve_id}_pdfplumber.md\")\n",
    "\n",
    "    # Mistral OCR\n",
    "    if normalized[\"mistral_ocr\"].get(eleve_id):\n",
    "        export_to_markdown(normalized[\"mistral_ocr\"][eleve_id], \"Mistral OCR\", BENCHMARK_DIR / f\"{eleve_id}_mistral_ocr.md\")\n",
    "\n",
    "    # Docling\n",
    "    if normalized[\"docling\"].get(eleve_id):\n",
    "        export_to_markdown(normalized[\"docling\"][eleve_id], \"Docling\", BENCHMARK_DIR / f\"{eleve_id}_docling.md\")\n",
    "\n",
    "    print(f\"✓ {eleve_id} exporté\")\n",
    "\n",
    "print(f\"\\nFichiers exportés dans: {BENCHMARK_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparaison avec le Ground Truth\n",
    "\n",
    "Fonction de comparaison générique pour les trois sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comparaisons: 138 lignes\n"
     ]
    }
   ],
   "source": [
    "def compare_with_gt(extracted: dict, gt: dict, source: str) -> pd.DataFrame:\n",
    "    \"\"\"Compare les données extraites avec le ground truth.\n",
    "\n",
    "    Args:\n",
    "        extracted: Données normalisées extraites\n",
    "        gt: Ground truth\n",
    "        source: Nom de la source (pdfplumber, mistral_ocr)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame avec les résultats de comparaison\n",
    "    \"\"\"\n",
    "    if not extracted:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    comparisons = []\n",
    "    gt_matieres = {m[\"nom\"]: m for m in gt[\"matieres\"]}\n",
    "\n",
    "    for m in extracted[\"matieres\"]:\n",
    "        nom = m[\"nom\"]\n",
    "        gt_m = gt_matieres.get(nom, {})\n",
    "\n",
    "        # Comparer moyennes\n",
    "        moy_eleve_ok = m[\"moy_eleve\"] == gt_m.get(\"moyenne_eleve\")\n",
    "        moy_classe_ok = m[\"moy_classe\"] == gt_m.get(\"moyenne_classe\")\n",
    "\n",
    "        # Comparer appréciations (normaliser les espaces)\n",
    "        app_ext = \" \".join((m.get(\"appreciation\") or \"\").split())\n",
    "        app_gt = \" \".join(gt_m.get(\"appreciation\", \"\").split())\n",
    "        app_ok = app_ext == app_gt\n",
    "\n",
    "        comparisons.append({\n",
    "            \"source\": source,\n",
    "            \"matiere\": nom,\n",
    "            \"moy_eleve_ext\": m[\"moy_eleve\"],\n",
    "            \"moy_eleve_gt\": gt_m.get(\"moyenne_eleve\"),\n",
    "            \"moy_eleve_ok\": \"✅\" if moy_eleve_ok else \"❌\",\n",
    "            \"moy_classe_ext\": m[\"moy_classe\"],\n",
    "            \"moy_classe_gt\": gt_m.get(\"moyenne_classe\"),\n",
    "            \"moy_classe_ok\": \"✅\" if moy_classe_ok else \"❌\",\n",
    "            \"app_ok\": \"✅\" if app_ok else \"❌\",\n",
    "            \"app_len_ext\": len(app_ext),\n",
    "            \"app_len_gt\": len(app_gt),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(comparisons)\n",
    "\n",
    "\n",
    "# Comparer toutes les sources pour tous les élèves\n",
    "all_comparisons = []\n",
    "\n",
    "for source_name, source_data in normalized.items():\n",
    "    for eleve_id, extracted in source_data.items():\n",
    "        df = compare_with_gt(extracted, gt_by_eleve[eleve_id], source_name)\n",
    "        if not df.empty:\n",
    "            df[\"eleve_id\"] = eleve_id\n",
    "            all_comparisons.append(df)\n",
    "\n",
    "df_comparison = pd.concat(all_comparisons, ignore_index=True)\n",
    "print(f\"Total comparaisons: {len(df_comparison)} lignes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STATISTIQUES PAR SOURCE\n",
      "============================================================\n",
      "     source  total_matieres  moy_eleve_%  moy_classe_%  app_%\n",
      " pdfplumber              46        100.0         100.0  100.0\n",
      "mistral_ocr              46        100.0         100.0  100.0\n",
      "    docling              46        100.0         100.0   37.0\n"
     ]
    }
   ],
   "source": [
    "# Statistiques par source\n",
    "stats_by_source = []\n",
    "\n",
    "for source in df_comparison[\"source\"].unique():\n",
    "    df_src = df_comparison[df_comparison[\"source\"] == source]\n",
    "    total = len(df_src)\n",
    "\n",
    "    stats_by_source.append({\n",
    "        \"source\": source,\n",
    "        \"total_matieres\": total,\n",
    "        \"moy_eleve_ok\": (df_src[\"moy_eleve_ok\"] == \"✅\").sum(),\n",
    "        \"moy_classe_ok\": (df_src[\"moy_classe_ok\"] == \"✅\").sum(),\n",
    "        \"app_ok\": (df_src[\"app_ok\"] == \"✅\").sum(),\n",
    "    })\n",
    "\n",
    "df_stats = pd.DataFrame(stats_by_source)\n",
    "df_stats[\"moy_eleve_%\"] = (df_stats[\"moy_eleve_ok\"] / df_stats[\"total_matieres\"] * 100).round(1)\n",
    "df_stats[\"moy_classe_%\"] = (df_stats[\"moy_classe_ok\"] / df_stats[\"total_matieres\"] * 100).round(1)\n",
    "df_stats[\"app_%\"] = (df_stats[\"app_ok\"] / df_stats[\"total_matieres\"] * 100).round(1)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STATISTIQUES PAR SOURCE\")\n",
    "print(\"=\" * 60)\n",
    "print(df_stats[[\"source\", \"total_matieres\", \"moy_eleve_%\", \"moy_classe_%\", \"app_%\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appréciations différentes (29):\n",
      "\n",
      " source eleve_id                 matiere  app_len_ext  app_len_gt\n",
      "docling  ELEVE_A             Anglais LV1          148         164\n",
      "docling  ELEVE_A                     EPS          266         291\n",
      "docling  ELEVE_A      Éducation Musicale           65          75\n",
      "docling  ELEVE_A                Français          169         184\n",
      "docling  ELEVE_A Histoire-Géographie-EMC          142         158\n",
      "docling  ELEVE_A           Mathématiques          188         204\n",
      "docling  ELEVE_B             Anglais LV1          207         219\n",
      "docling  ELEVE_B                     EPS           86          91\n",
      "docling  ELEVE_B                Français          168         182\n",
      "docling  ELEVE_B             Italien LV2          250         268\n",
      "docling  ELEVE_B           Mathématiques          211         226\n",
      "docling  ELEVE_B         Physique-Chimie           92         106\n",
      "docling  ELEVE_B                     SVT          149         162\n",
      "docling  ELEVE_B             Technologie           63          78\n",
      "docling  ELEVE_C             Anglais LV1          191         201\n",
      "docling  ELEVE_C         Arts Plastiques           80          92\n",
      "docling  ELEVE_C                     EPS          143         146\n",
      "docling  ELEVE_C                Français          113         121\n",
      "docling  ELEVE_C           Mathématiques          281         340\n",
      "docling  ELEVE_C                     SVT          160         172\n",
      "docling  ELEVE_D             Anglais LV1          222         239\n",
      "docling  ELEVE_D         Arts Plastiques           81          94\n",
      "docling  ELEVE_D                     EPS          203         222\n",
      "docling  ELEVE_D      Éducation Musicale          117         120\n",
      "docling  ELEVE_D                Français          268         288\n",
      "docling  ELEVE_D Histoire-Géographie-EMC          195         209\n",
      "docling  ELEVE_D           Mathématiques          216         234\n",
      "docling  ELEVE_D                     SVT          160         170\n",
      "docling  ELEVE_D             Technologie           70          85\n"
     ]
    }
   ],
   "source": [
    "# Détail des appréciations qui ne matchent pas\n",
    "df_errors = df_comparison[df_comparison[\"app_ok\"] == \"❌\"][\n",
    "    [\"source\", \"eleve_id\", \"matiere\", \"app_len_ext\", \"app_len_gt\"]\n",
    "]\n",
    "print(f\"Appréciations différentes ({len(df_errors)}):\\n\")\n",
    "if not df_errors.empty:\n",
    "    print(df_errors.to_string(index=False))\n",
    "else:\n",
    "    print(\"Aucune erreur d'appréciation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Résumé comparatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution (secondes):\n",
      "eleve_id  pdfplumber_time  mistral_ocr_time  docling_time\n",
      " ELEVE_A         0.063753          1.960890     22.323617\n",
      " ELEVE_B         0.062796          2.079989      4.443762\n",
      " ELEVE_C         0.057298          1.724445      4.601950\n",
      " ELEVE_D         0.068962          1.897707      4.846486\n",
      "\n",
      "Moyenne pdfplumber: 0.06s\n",
      "Moyenne Mistral OCR: 1.92s\n",
      "Moyenne Docling: 9.05s\n"
     ]
    }
   ],
   "source": [
    "# Temps d'exécution\n",
    "df_times = pd.DataFrame([\n",
    "    {\n",
    "        \"eleve_id\": eleve_id,\n",
    "        \"pdfplumber_time\": pdfplumber_times.get(eleve_id),\n",
    "        \"mistral_ocr_time\": mistral_times.get(eleve_id),\n",
    "        \"docling_time\": docling_times.get(eleve_id),\n",
    "    }\n",
    "    for eleve_id in gt_by_eleve.keys()\n",
    "])\n",
    "\n",
    "print(\"Temps d'exécution (secondes):\")\n",
    "print(df_times.to_string(index=False))\n",
    "print(f\"\\nMoyenne pdfplumber: {df_times['pdfplumber_time'].mean():.2f}s\")\n",
    "print(f\"Moyenne Mistral OCR: {df_times['mistral_ocr_time'].mean():.2f}s\")\n",
    "print(f\"Moyenne Docling: {df_times['docling_time'].mean():.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RÉSUMÉ BENCHMARK\n",
      "================================================================================\n",
      "                 Métrique pdfplumber mistral_ocr docling\n",
      "          Temps moyen (s)       0.06        1.92    9.05\n",
      " Moyennes élève correctes     100.0%      100.0%  100.0%\n",
      "Moyennes classe correctes     100.0%      100.0%  100.0%\n",
      " Appréciations identiques     100.0%      100.0%   37.0%\n",
      "\n",
      "================================================================================\n",
      "CONCLUSION\n",
      "================================================================================\n",
      "\n",
      "## Résultats du benchmark\n",
      "\n",
      "**Mistral OCR** : 100% de précision sur notes et appréciations\n",
      "- Interprète correctement la structure sémantique\n",
      "- Post-traitement minimal requis\n",
      "\n",
      "**pdfplumber** : 100% sur notes, nécessite post-traitement\n",
      "- Extraction mécanique (pas de ML)\n",
      "- Headers multi-lignes mal gérés sans normalisation\n",
      "- Maintenance requise si le format change\n",
      "\n",
      "**Docling (IBM)** : NON RETENU\n",
      "- Bug connu : perte de mots aux retours à la ligne dans les cellules\n",
      "- Malgré une table simple (3 colonnes, pas de fusion), le modèle\n",
      "  TableFormer tronque le texte (ex: \"Poursuivez ainsi!\" → \"ainsi!\")\n",
      "- Issues GitHub #1922, #2064 documentent ce problème\n",
      "\n",
      "## Autres solutions évaluées (non testées)\n",
      "\n",
      "Solutions locales open source considérées mais non retenues :\n",
      "- **PDF-Extract-Kit / MinerU** : Complet mais complexe (modèles différents par type)\n",
      "- **olmOCR** (Allen AI) : Prometteur mais installation lourde\n",
      "- **PaddleOCR** (Baidu) : Bon sur tableaux mais expertise requise\n",
      "- **Marker** : PDF→Markdown, moins mature\n",
      "\n",
      "Ces solutions demandent une expertise OCR et une maintenance\n",
      "qui dépassent le cadre du projet Chiron.\n",
      "\n",
      "## Choix final : Mistral OCR\n",
      "\n",
      "Raisons :\n",
      "- Précision 100% sans post-traitement complexe\n",
      "- API simple (5 lignes de code)\n",
      "- Gère PDF et images de manière uniforme\n",
      "- Robuste face aux variations de format\n",
      "- Pas de maintenance de modèles ou règles spécifiques\n",
      "- Coût négligeable (~0.001€/page, <1€/trimestre pour Chiron)\n",
      "\n",
      "Le temps économisé sur la configuration et maintenance d'une solution\n",
      "locale justifie largement le coût minime de l'API.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tableau récapitulatif final\n",
    "print(\"=\" * 80)\n",
    "print(\"RÉSUMÉ BENCHMARK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Helper pour récupérer les stats d'une source\n",
    "def get_stats(source):\n",
    "    row = df_stats[df_stats[\"source\"] == source]\n",
    "    if row.empty:\n",
    "        return {\"moy_eleve_%\": \"N/A\", \"moy_classe_%\": \"N/A\", \"app_%\": \"N/A\"}\n",
    "    return row.iloc[0]\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"Métrique\": [\n",
    "        \"Temps moyen (s)\",\n",
    "        \"Moyennes élève correctes\",\n",
    "        \"Moyennes classe correctes\",\n",
    "        \"Appréciations identiques\",\n",
    "    ],\n",
    "    \"pdfplumber\": [\n",
    "        f\"{df_times['pdfplumber_time'].mean():.2f}\",\n",
    "        f\"{get_stats('pdfplumber')['moy_eleve_%']}%\",\n",
    "        f\"{get_stats('pdfplumber')['moy_classe_%']}%\",\n",
    "        f\"{get_stats('pdfplumber')['app_%']}%\",\n",
    "    ],\n",
    "    \"mistral_ocr\": [\n",
    "        f\"{df_times['mistral_ocr_time'].mean():.2f}\",\n",
    "        f\"{get_stats('mistral_ocr')['moy_eleve_%']}%\",\n",
    "        f\"{get_stats('mistral_ocr')['moy_classe_%']}%\",\n",
    "        f\"{get_stats('mistral_ocr')['app_%']}%\",\n",
    "    ],\n",
    "    \"docling\": [\n",
    "        f\"{df_times['docling_time'].mean():.2f}\",\n",
    "        f\"{get_stats('docling')['moy_eleve_%']}%\",\n",
    "        f\"{get_stats('docling')['moy_classe_%']}%\",\n",
    "        f\"{get_stats('docling')['app_%']}%\",\n",
    "    ],\n",
    "})\n",
    "\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "## Résultats du benchmark\n",
    "\n",
    "**Mistral OCR** : 100% de précision sur notes et appréciations\n",
    "- Interprète correctement la structure sémantique\n",
    "- Post-traitement minimal requis\n",
    "\n",
    "**pdfplumber** : 100% sur notes, nécessite post-traitement\n",
    "- Extraction mécanique (pas de ML)\n",
    "- Headers multi-lignes mal gérés sans normalisation\n",
    "- Maintenance requise si le format change\n",
    "\n",
    "**Docling (IBM)** : NON RETENU\n",
    "- Bug connu : perte de mots aux retours à la ligne dans les cellules\n",
    "- Malgré une table simple (3 colonnes, pas de fusion), le modèle\n",
    "  TableFormer tronque le texte (ex: \"Poursuivez ainsi!\" → \"ainsi!\")\n",
    "- Issues GitHub #1922, #2064 documentent ce problème\n",
    "\n",
    "## Autres solutions évaluées (non testées)\n",
    "\n",
    "Solutions locales open source considérées mais non retenues :\n",
    "- **PDF-Extract-Kit / MinerU** : Complet mais complexe (modèles différents par type)\n",
    "- **olmOCR** (Allen AI) : Prometteur mais installation lourde\n",
    "- **PaddleOCR** (Baidu) : Bon sur tableaux mais expertise requise\n",
    "- **Marker** : PDF→Markdown, moins mature\n",
    "\n",
    "Ces solutions demandent une expertise OCR et une maintenance\n",
    "qui dépassent le cadre du projet Chiron.\n",
    "\n",
    "## Choix final : Mistral OCR\n",
    "\n",
    "Raisons :\n",
    "- Précision 100% sans post-traitement complexe\n",
    "- API simple (5 lignes de code)\n",
    "- Gère PDF et images de manière uniforme\n",
    "- Robuste face aux variations de format\n",
    "- Pas de maintenance de modèles ou règles spécifiques\n",
    "- Coût négligeable (~0.001€/page, <1€/trimestre pour Chiron)\n",
    "\n",
    "Le temps économisé sur la configuration et maintenance d'une solution\n",
    "locale justifie largement le coût minime de l'API.\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chiron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
