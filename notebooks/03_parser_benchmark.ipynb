{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Parser Benchmark\n",
    "\n",
    "Comparaison des méthodes d'extraction PDF :\n",
    "- **pdfplumber** : Parser actuel (extraction de tableaux)\n",
    "- **Mistral OCR** : Vision model (mistral-ocr-2503)\n",
    "\n",
    "## Objectif\n",
    "Mesurer la précision d'extraction des bulletins scolaires vs ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\Florent\\Documents\\data_science\\chiron\n",
      "PDFs: [WindowsPath('c:/Users/Florent/Documents/data_science/chiron/data/raw/ELEVE_A.pdf'), WindowsPath('c:/Users/Florent/Documents/data_science/chiron/data/raw/ELEVE_B.pdf'), WindowsPath('c:/Users/Florent/Documents/data_science/chiron/data/raw/ELEVE_C.pdf'), WindowsPath('c:/Users/Florent/Documents/data_science/chiron/data/raw/ELEVE_D.pdf')]\n"
     ]
    }
   ],
   "source": [
    "# ruff: noqa: E402\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Auto-détecter project_root\n",
    "current = Path.cwd()\n",
    "while current != current.parent:\n",
    "    if (current / \"pyproject.toml\").exists():\n",
    "        project_root = current\n",
    "        break\n",
    "    current = current.parent\n",
    "\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(project_root / \".env\")\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = project_root / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "GROUND_TRUTH_PATH = DATA_DIR / \"ground_truth\" / \"chiron_ground_truth.json\"\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PDFs: {list(RAW_DIR.glob('*.pdf'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: ['ELEVE_A', 'ELEVE_B', 'ELEVE_C', 'ELEVE_D']\n"
     ]
    }
   ],
   "source": [
    "# Charger ground truth\n",
    "with open(GROUND_TRUTH_PATH, encoding=\"utf-8\") as f:\n",
    "    ground_truth = json.load(f)\n",
    "\n",
    "# Index par eleve_id pour comparaison facile\n",
    "gt_by_eleve = {e[\"eleve_id\"]: e for e in ground_truth[\"eleves\"]}\n",
    "print(f\"Ground truth: {list(gt_by_eleve.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parser pdfplumber (actuel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELEVE_A: 0.06s\n",
      "ELEVE_B: 0.06s\n",
      "ELEVE_C: 0.06s\n",
      "ELEVE_D: 0.07s\n"
     ]
    }
   ],
   "source": [
    "from src.document.bulletin_parser import BulletinParser\n",
    "\n",
    "parser = BulletinParser()\n",
    "\n",
    "pdfplumber_results = {}\n",
    "pdfplumber_times = {}\n",
    "\n",
    "for pdf_path in sorted(RAW_DIR.glob(\"*.pdf\")):\n",
    "    eleve_id = pdf_path.stem  # ELEVE_A, ELEVE_B, etc.\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    try:\n",
    "        eleves = parser.parse(pdf_path)\n",
    "        pdfplumber_results[eleve_id] = eleves[0] if eleves else None\n",
    "    except Exception as e:\n",
    "        pdfplumber_results[eleve_id] = f\"ERROR: {e}\"\n",
    "    pdfplumber_times[eleve_id] = time.perf_counter() - start\n",
    "\n",
    "    print(f\"{eleve_id}: {pdfplumber_times[eleve_id]:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nom: None\n",
      "Prénom: None\n",
      "Classe: None\n",
      "Matières extraites: 12\n",
      "  - Anglais LV1: 15.21\n",
      "  - Arts Plastiques: 15.0\n",
      "  - EPS: 8.0\n",
      "  - Éducation Musicale: 13.0\n",
      "  - Espagnol LV2: 13.83\n",
      "  - Français: 13.21\n",
      "  - Histoire-Géographie-EMC: 14.66\n",
      "  - Latin et Grec: 18.21\n",
      "  - Mathématiques: 16.07\n",
      "  - Physique-Chimie: 15.71\n",
      "  - SVT: 10.86\n",
      "  - Technologie: 15.8\n"
     ]
    }
   ],
   "source": [
    "# Afficher un exemple de résultat pdfplumber\n",
    "exemple = pdfplumber_results.get(\"ELEVE_A\")\n",
    "if exemple and not isinstance(exemple, str):\n",
    "    print(f\"Nom: {exemple.nom}\")\n",
    "    print(f\"Prénom: {exemple.prenom}\")\n",
    "    print(f\"Classe: {exemple.classe}\")\n",
    "    print(f\"Matières extraites: {len(exemple.matieres)}\")\n",
    "    for m in exemple.matieres[:]:\n",
    "        print(f\"  - {m.nom}: {m.moyenne_eleve}\")\n",
    "else:\n",
    "    print(f\"Erreur ou pas de résultat: {exemple}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mistral OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client Mistral initialisé\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "from mistralai import Mistral\n",
    "\n",
    "# Init client\n",
    "mistral_api_key = os.getenv(\"MISTRAL_OCR_API_KEY\")\n",
    "if not mistral_api_key:\n",
    "    raise ValueError(\"MISTRAL_OCR_API_KEY non configurée dans .env\")\n",
    "\n",
    "client = Mistral(api_key=mistral_api_key)\n",
    "print(\"Client Mistral initialisé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_with_mistral_ocr(pdf_path: Path) -> dict:\n",
    "    \"\"\"Extrait le contenu d'un PDF avec Mistral OCR.\n",
    "\n",
    "    Utilise l'API OCR de Mistral (mistral-ocr-2503).\n",
    "    Ref: https://docs.mistral.ai/capabilities/document/\n",
    "    \"\"\"\n",
    "    # Encoder le PDF en base64\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        pdf_base64 = base64.standard_b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "    # Appel API OCR\n",
    "    response = client.ocr.process(\n",
    "        model=\"mistral-ocr-2503\",\n",
    "        document={\n",
    "            \"type\": \"document_url\",\n",
    "            \"document_url\": f\"data:application/pdf;base64,{pdf_base64}\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELEVE_A: 2.41s\n",
      "ELEVE_B: 2.26s\n",
      "ELEVE_C: 2.30s\n",
      "ELEVE_D: 2.42s\n"
     ]
    }
   ],
   "source": [
    "# Extraire tous les PDFs avec Mistral OCR\n",
    "mistral_results = {}\n",
    "mistral_times = {}\n",
    "\n",
    "for pdf_path in sorted(RAW_DIR.glob(\"*.pdf\")):\n",
    "    eleve_id = pdf_path.stem\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    try:\n",
    "        result = extract_with_mistral_ocr(pdf_path)\n",
    "        mistral_results[eleve_id] = result\n",
    "    except Exception as e:\n",
    "        mistral_results[eleve_id] = f\"ERROR: {e}\"\n",
    "    mistral_times[eleve_id] = time.perf_counter() - start\n",
    "\n",
    "    print(f\"{eleve_id}: {mistral_times[eleve_id]:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalisation des extractions\n",
    "\n",
    "Convertir les résultats des deux parsers en format uniforme pour comparaison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données normalisées pour:\n",
      "  - pdfplumber: ['ELEVE_A', 'ELEVE_B', 'ELEVE_C', 'ELEVE_D']\n",
      "  - mistral_ocr: ['ELEVE_A', 'ELEVE_B', 'ELEVE_C', 'ELEVE_D']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def parse_mistral_markdown(markdown: str) -> dict:\n",
    "    \"\"\"Parse le markdown de Mistral OCR en structure normalisée.\"\"\"\n",
    "    result = {\n",
    "        \"eleve_id\": None,\n",
    "        \"genre\": None,\n",
    "        \"absences\": None,\n",
    "        \"engagements\": None,\n",
    "        \"matieres\": [],\n",
    "        \"moyenne_generale\": None,\n",
    "    }\n",
    "\n",
    "    eleve_match = re.search(r\"Élève\\s*:\\s*(\\w+)\", markdown)\n",
    "    if eleve_match:\n",
    "        result[\"eleve_id\"] = eleve_match.group(1)\n",
    "\n",
    "    genre_match = re.search(r\"Genre\\s*:\\s*(\\w+)\", markdown)\n",
    "    if genre_match:\n",
    "        result[\"genre\"] = genre_match.group(1)\n",
    "\n",
    "    absences_match = re.search(r\"Absences\\s*:\\s*(\\d+)\", markdown)\n",
    "    if absences_match:\n",
    "        result[\"absences\"] = int(absences_match.group(1))\n",
    "\n",
    "    engagements_match = re.search(r\"Engagements\\s*:\\s*([^<\\n]+)\", markdown)\n",
    "    if engagements_match:\n",
    "        result[\"engagements\"] = engagements_match.group(1).strip()\n",
    "\n",
    "    table_pattern = r\"\\|\\s*([^|]+)\\s*\\|\\s*([^|]+)\\s*\\|\\s*([^|]*)\\s*\\|\"\n",
    "    rows = re.findall(table_pattern, markdown)\n",
    "\n",
    "    for row in rows:\n",
    "        matiere, notes, appreciation = [cell.strip() for cell in row]\n",
    "        if matiere in (\"Matière\", \":--:\") or \"Élève\" in matiere:\n",
    "            continue\n",
    "        notes_match = re.search(r\"([\\d.]+)\\s*/\\s*([\\d.]+)\", notes)\n",
    "        result[\"matieres\"].append({\n",
    "            \"nom\": matiere,\n",
    "            \"moy_eleve\": float(notes_match.group(1)) if notes_match else None,\n",
    "            \"moy_classe\": float(notes_match.group(2)) if notes_match else None,\n",
    "            \"appreciation\": appreciation,\n",
    "        })\n",
    "\n",
    "    moy_match = re.search(r\"Moyenne générale\\s*:\\s*([\\d.]+)\", markdown)\n",
    "    if moy_match:\n",
    "        result[\"moyenne_generale\"] = float(moy_match.group(1))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def normalize_pdfplumber(result) -> dict:\n",
    "    \"\"\"Convertit le résultat pdfplumber en structure normalisée.\"\"\"\n",
    "    if isinstance(result, str) or result is None:\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        \"eleve_id\": result.nom,\n",
    "        \"genre\": result.genre,\n",
    "        \"absences\": result.absences_demi_journees,\n",
    "        \"engagements\": \", \".join(result.engagements) if result.engagements else None,\n",
    "        \"matieres\": [\n",
    "            {\n",
    "                \"nom\": m.nom,\n",
    "                \"moy_eleve\": m.moyenne_eleve,\n",
    "                \"moy_classe\": m.moyenne_classe,\n",
    "                \"appreciation\": m.appreciation,\n",
    "            }\n",
    "            for m in result.matieres\n",
    "        ],\n",
    "        \"moyenne_generale\": None,  # Non extrait par pdfplumber\n",
    "    }\n",
    "\n",
    "\n",
    "# Normaliser tous les résultats\n",
    "normalized = {\"pdfplumber\": {}, \"mistral_ocr\": {}}\n",
    "\n",
    "for eleve_id in gt_by_eleve.keys():\n",
    "    # pdfplumber\n",
    "    pdf_result = pdfplumber_results.get(eleve_id)\n",
    "    normalized[\"pdfplumber\"][eleve_id] = normalize_pdfplumber(pdf_result)\n",
    "\n",
    "    # Mistral OCR\n",
    "    mistral_result = mistral_results.get(eleve_id)\n",
    "    if mistral_result and not isinstance(mistral_result, str):\n",
    "        normalized[\"mistral_ocr\"][eleve_id] = parse_mistral_markdown(\n",
    "            mistral_result.pages[0].markdown\n",
    "        )\n",
    "\n",
    "print(\"Données normalisées pour:\")\n",
    "for source, data in normalized.items():\n",
    "    print(f\"  - {source}: {list(data.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Export des extractions\n",
    "\n",
    "Sauvegarder les extractions en markdown pour faciliter la comparaison visuelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ELEVE_A exporté\n",
      "✓ ELEVE_B exporté\n",
      "✓ ELEVE_C exporté\n",
      "✓ ELEVE_D exporté\n",
      "\n",
      "Fichiers exportés dans: c:\\Users\\Florent\\Documents\\data_science\\chiron\\data\\processed\\benchmark-extraction\n"
     ]
    }
   ],
   "source": [
    "BENCHMARK_DIR = DATA_DIR / \"processed\" / \"benchmark-extraction\"\n",
    "BENCHMARK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def export_to_markdown(data: dict, source: str, output_path: Path):\n",
    "    \"\"\"Exporte les données normalisées en markdown.\"\"\"\n",
    "    lines = [\n",
    "        f\"# {source} - {data.get('eleve_id', 'N/A')}\",\n",
    "        \"\",\n",
    "        f\"**Genre:** {data.get('genre', 'N/A')}\",\n",
    "        f\"**Absences:** {data.get('absences', 'N/A')} demi-journées\",\n",
    "        f\"**Engagements:** {data.get('engagements') or 'Aucun'}\",\n",
    "        \"\",\n",
    "        \"## Matières\",\n",
    "        \"\",\n",
    "        \"| Matière | Moy. Élève | Moy. Classe | Appréciation |\",\n",
    "        \"|---------|------------|-------------|--------------|\",\n",
    "    ]\n",
    "\n",
    "    for m in data.get(\"matieres\", []):\n",
    "        app = (m.get(\"appreciation\") or \"\").replace(\"|\", \"\\\\|\").replace(\"\\n\", \" \")\n",
    "        lines.append(\n",
    "            f\"| {m['nom']} | {m['moy_eleve']} | {m['moy_classe']} | {app} |\"\n",
    "        )\n",
    "\n",
    "    if data.get(\"moyenne_generale\"):\n",
    "        lines.extend([\"\", f\"**Moyenne générale:** {data['moyenne_generale']}/20\"])\n",
    "\n",
    "    output_path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# Exporter pour tous les élèves\n",
    "for eleve_id in gt_by_eleve.keys():\n",
    "    # Ground truth\n",
    "    gt_data = {\n",
    "        \"eleve_id\": eleve_id,\n",
    "        \"genre\": gt_by_eleve[eleve_id][\"genre\"],\n",
    "        \"absences\": gt_by_eleve[eleve_id][\"absences_demi_journees\"],\n",
    "        \"engagements\": \", \".join(gt_by_eleve[eleve_id].get(\"engagements\", [])),\n",
    "        \"matieres\": [\n",
    "            {\n",
    "                \"nom\": m[\"nom\"],\n",
    "                \"moy_eleve\": m[\"moyenne_eleve\"],\n",
    "                \"moy_classe\": m[\"moyenne_classe\"],\n",
    "                \"appreciation\": m.get(\"appreciation\", \"\"),\n",
    "            }\n",
    "            for m in gt_by_eleve[eleve_id][\"matieres\"]\n",
    "        ],\n",
    "    }\n",
    "    export_to_markdown(gt_data, \"Ground Truth\", BENCHMARK_DIR / f\"{eleve_id}_ground_truth.md\")\n",
    "\n",
    "    # pdfplumber\n",
    "    if normalized[\"pdfplumber\"].get(eleve_id):\n",
    "        export_to_markdown(\n",
    "            normalized[\"pdfplumber\"][eleve_id],\n",
    "            \"pdfplumber\",\n",
    "            BENCHMARK_DIR / f\"{eleve_id}_pdfplumber.md\",\n",
    "        )\n",
    "\n",
    "    # Mistral OCR\n",
    "    if normalized[\"mistral_ocr\"].get(eleve_id):\n",
    "        export_to_markdown(\n",
    "            normalized[\"mistral_ocr\"][eleve_id],\n",
    "            \"Mistral OCR\",\n",
    "            BENCHMARK_DIR / f\"{eleve_id}_mistral_ocr.md\",\n",
    "        )\n",
    "\n",
    "    print(f\"✓ {eleve_id} exporté\")\n",
    "\n",
    "print(f\"\\nFichiers exportés dans: {BENCHMARK_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparaison avec le Ground Truth\n",
    "\n",
    "Fonction de comparaison générique pour les deux sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comparaisons: 92 lignes\n"
     ]
    }
   ],
   "source": [
    "def compare_with_gt(extracted: dict, gt: dict, source: str) -> pd.DataFrame:\n",
    "    \"\"\"Compare les données extraites avec le ground truth.\n",
    "\n",
    "    Args:\n",
    "        extracted: Données normalisées extraites\n",
    "        gt: Ground truth\n",
    "        source: Nom de la source (pdfplumber, mistral_ocr)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame avec les résultats de comparaison\n",
    "    \"\"\"\n",
    "    if not extracted:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    comparisons = []\n",
    "    gt_matieres = {m[\"nom\"]: m for m in gt[\"matieres\"]}\n",
    "\n",
    "    for m in extracted[\"matieres\"]:\n",
    "        nom = m[\"nom\"]\n",
    "        gt_m = gt_matieres.get(nom, {})\n",
    "\n",
    "        # Comparer moyennes\n",
    "        moy_eleve_ok = m[\"moy_eleve\"] == gt_m.get(\"moyenne_eleve\")\n",
    "        moy_classe_ok = m[\"moy_classe\"] == gt_m.get(\"moyenne_classe\")\n",
    "\n",
    "        # Comparer appréciations (normaliser les espaces)\n",
    "        app_ext = \" \".join((m.get(\"appreciation\") or \"\").split())\n",
    "        app_gt = \" \".join(gt_m.get(\"appreciation\", \"\").split())\n",
    "        app_ok = app_ext == app_gt\n",
    "\n",
    "        comparisons.append({\n",
    "            \"source\": source,\n",
    "            \"matiere\": nom,\n",
    "            \"moy_eleve_ext\": m[\"moy_eleve\"],\n",
    "            \"moy_eleve_gt\": gt_m.get(\"moyenne_eleve\"),\n",
    "            \"moy_eleve_ok\": \"✅\" if moy_eleve_ok else \"❌\",\n",
    "            \"moy_classe_ext\": m[\"moy_classe\"],\n",
    "            \"moy_classe_gt\": gt_m.get(\"moyenne_classe\"),\n",
    "            \"moy_classe_ok\": \"✅\" if moy_classe_ok else \"❌\",\n",
    "            \"app_ok\": \"✅\" if app_ok else \"❌\",\n",
    "            \"app_len_ext\": len(app_ext),\n",
    "            \"app_len_gt\": len(app_gt),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(comparisons)\n",
    "\n",
    "\n",
    "# Comparer toutes les sources pour tous les élèves\n",
    "all_comparisons = []\n",
    "\n",
    "for source_name, source_data in normalized.items():\n",
    "    for eleve_id, extracted in source_data.items():\n",
    "        df = compare_with_gt(extracted, gt_by_eleve[eleve_id], source_name)\n",
    "        if not df.empty:\n",
    "            df[\"eleve_id\"] = eleve_id\n",
    "            all_comparisons.append(df)\n",
    "\n",
    "df_comparison = pd.concat(all_comparisons, ignore_index=True)\n",
    "print(f\"Total comparaisons: {len(df_comparison)} lignes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STATISTIQUES PAR SOURCE\n",
      "============================================================\n",
      "     source  total_matieres  moy_eleve_%  moy_classe_%  app_%\n",
      " pdfplumber              46        100.0         100.0  100.0\n",
      "mistral_ocr              46        100.0         100.0   93.5\n"
     ]
    }
   ],
   "source": [
    "# Statistiques par source\n",
    "stats_by_source = []\n",
    "\n",
    "for source in df_comparison[\"source\"].unique():\n",
    "    df_src = df_comparison[df_comparison[\"source\"] == source]\n",
    "    total = len(df_src)\n",
    "\n",
    "    stats_by_source.append({\n",
    "        \"source\": source,\n",
    "        \"total_matieres\": total,\n",
    "        \"moy_eleve_ok\": (df_src[\"moy_eleve_ok\"] == \"✅\").sum(),\n",
    "        \"moy_classe_ok\": (df_src[\"moy_classe_ok\"] == \"✅\").sum(),\n",
    "        \"app_ok\": (df_src[\"app_ok\"] == \"✅\").sum(),\n",
    "    })\n",
    "\n",
    "df_stats = pd.DataFrame(stats_by_source)\n",
    "df_stats[\"moy_eleve_%\"] = (df_stats[\"moy_eleve_ok\"] / df_stats[\"total_matieres\"] * 100).round(1)\n",
    "df_stats[\"moy_classe_%\"] = (df_stats[\"moy_classe_ok\"] / df_stats[\"total_matieres\"] * 100).round(1)\n",
    "df_stats[\"app_%\"] = (df_stats[\"app_ok\"] / df_stats[\"total_matieres\"] * 100).round(1)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STATISTIQUES PAR SOURCE\")\n",
    "print(\"=\" * 60)\n",
    "print(df_stats[[\"source\", \"total_matieres\", \"moy_eleve_%\", \"moy_classe_%\", \"app_%\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appréciations différentes (3):\n",
      "\n",
      "     source eleve_id                 matiere  app_len_ext  app_len_gt\n",
      "mistral_ocr  ELEVE_B             Anglais LV1          218         219\n",
      "mistral_ocr  ELEVE_B                     EPS           90          91\n",
      "mistral_ocr  ELEVE_C Histoire-Géographie-EMC           56          56\n"
     ]
    }
   ],
   "source": [
    "# Détail des appréciations qui ne matchent pas\n",
    "df_errors = df_comparison[df_comparison[\"app_ok\"] == \"❌\"][\n",
    "    [\"source\", \"eleve_id\", \"matiere\", \"app_len_ext\", \"app_len_gt\"]\n",
    "]\n",
    "print(f\"Appréciations différentes ({len(df_errors)}):\\n\")\n",
    "if not df_errors.empty:\n",
    "    print(df_errors.to_string(index=False))\n",
    "else:\n",
    "    print(\"Aucune erreur d'appréciation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Résumé comparatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution (secondes):\n",
      "eleve_id  pdfplumber_time  mistral_ocr_time\n",
      " ELEVE_A         0.064094          2.410624\n",
      " ELEVE_B         0.062869          2.256400\n",
      " ELEVE_C         0.055212          2.299399\n",
      " ELEVE_D         0.065765          2.416317\n",
      "\n",
      "Moyenne pdfplumber: 0.06s\n",
      "Moyenne Mistral OCR: 2.35s\n",
      "Ratio: 38x plus lent\n"
     ]
    }
   ],
   "source": [
    "# Temps d'exécution\n",
    "df_times = pd.DataFrame([\n",
    "    {\n",
    "        \"eleve_id\": eleve_id,\n",
    "        \"pdfplumber_time\": pdfplumber_times.get(eleve_id),\n",
    "        \"mistral_ocr_time\": mistral_times.get(eleve_id),\n",
    "    }\n",
    "    for eleve_id in gt_by_eleve.keys()\n",
    "])\n",
    "\n",
    "print(\"Temps d'exécution (secondes):\")\n",
    "print(df_times.to_string(index=False))\n",
    "print(f\"\\nMoyenne pdfplumber: {df_times['pdfplumber_time'].mean():.2f}s\")\n",
    "print(f\"Moyenne Mistral OCR: {df_times['mistral_ocr_time'].mean():.2f}s\")\n",
    "print(f\"Ratio: {df_times['mistral_ocr_time'].mean() / df_times['pdfplumber_time'].mean():.0f}x plus lent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RÉSUMÉ BENCHMARK\n",
      "======================================================================\n",
      "                 Métrique pdfplumber mistral_ocr\n",
      "          Temps moyen (s)       0.06        2.35\n",
      " Moyennes élève correctes     100.0%      100.0%\n",
      "Moyennes classe correctes     100.0%      100.0%\n",
      " Appréciations identiques     100.0%       93.5%\n",
      "\n",
      "======================================================================\n",
      "CONCLUSION\n",
      "======================================================================\n",
      "\n",
      "- pdfplumber est ~35x plus rapide que Mistral OCR\n",
      "- Les deux méthodes extraient correctement les notes numériques (100%)\n",
      "- pdfplumber a une meilleure précision sur les appréciations\n",
      "- Mistral OCR retourne du markdown structuré (utile pour d'autres formats)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tableau récapitulatif final\n",
    "print(\"=\" * 70)\n",
    "print(\"RÉSUMÉ BENCHMARK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"Métrique\": [\n",
    "        \"Temps moyen (s)\",\n",
    "        \"Moyennes élève correctes\",\n",
    "        \"Moyennes classe correctes\",\n",
    "        \"Appréciations identiques\",\n",
    "    ],\n",
    "    \"pdfplumber\": [\n",
    "        f\"{df_times['pdfplumber_time'].mean():.2f}\",\n",
    "        f\"{df_stats[df_stats['source'] == 'pdfplumber']['moy_eleve_%'].values[0]}%\",\n",
    "        f\"{df_stats[df_stats['source'] == 'pdfplumber']['moy_classe_%'].values[0]}%\",\n",
    "        f\"{df_stats[df_stats['source'] == 'pdfplumber']['app_%'].values[0]}%\",\n",
    "    ],\n",
    "    \"mistral_ocr\": [\n",
    "        f\"{df_times['mistral_ocr_time'].mean():.2f}\",\n",
    "        f\"{df_stats[df_stats['source'] == 'mistral_ocr']['moy_eleve_%'].values[0]}%\",\n",
    "        f\"{df_stats[df_stats['source'] == 'mistral_ocr']['moy_classe_%'].values[0]}%\",\n",
    "        f\"{df_stats[df_stats['source'] == 'mistral_ocr']['app_%'].values[0]}%\",\n",
    "    ],\n",
    "})\n",
    "\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "- pdfplumber est ~35x plus rapide que Mistral OCR\n",
    "- Les deux méthodes extraient correctement les notes numériques (100%)\n",
    "- pdfplumber a une meilleure précision sur les appréciations\n",
    "- Mistral OCR retourne du markdown structuré (utile pour d'autres formats)\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chiron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
