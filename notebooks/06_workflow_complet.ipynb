{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Workflow Complet Chiron\n",
    "\n",
    "Ce notebook montre le flow complet de production :\n",
    "\n",
    "```\n",
    "PDF original\n",
    "    |\n",
    "    v\n",
    "1. Extraction nom (pdfplumber + regex)      [local]\n",
    "    |\n",
    "    v\n",
    "2. NER variantes (CamemBERT)                [local, ~0.4s]\n",
    "    |\n",
    "    v\n",
    "3. Anonymisation PDF (PyMuPDF)              [local]\n",
    "    |\n",
    "    v\n",
    "4. OCR (Mistral OCR)                        [cloud, ~2s]\n",
    "    |\n",
    "    v\n",
    "5. Parsing structure (EleveExtraction)      [local]\n",
    "    |\n",
    "    v\n",
    "6. Stockage DuckDB                          [local]\n",
    "    |\n",
    "    v\n",
    "7. Generation synthese (LLM)                [cloud, ~5s]\n",
    "    |\n",
    "    v\n",
    "8. Export CSV                               [local]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: E402\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Auto-detecter project_root\n",
    "current = Path.cwd()\n",
    "while current != current.parent:\n",
    "    if (current / \"pyproject.toml\").exists():\n",
    "        project_root = current\n",
    "        break\n",
    "    current = current.parent\n",
    "\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(project_root / \".env\")\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = project_root / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "DB_DIR = DATA_DIR / \"db\"\n",
    "DB_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PDFs disponibles: {[p.name for p in RAW_DIR.glob('*.pdf')]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Affichage de la configuration centralisee depuis `src/llm/config.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.document import estimate_mistral_cost\n",
    "from src.llm.config import settings\n",
    "from src.llm.pricing import estimate_synthese_cost\n",
    "\n",
    "print(\"=== Configuration LLM ===\")\n",
    "print(f\"OCR Model     : {settings.mistral_ocr_model}\")\n",
    "print(f\"NER Model     : {settings.ner_model}\")\n",
    "print(f\"LLM Provider  : OpenAI (default: {settings.default_openai_model})\")\n",
    "print(f\"Temperature   : {settings.default_temperature}\")\n",
    "print(f\"Max tokens    : {settings.synthese_max_tokens}\")\n",
    "print()\n",
    "\n",
    "# Tarifs\n",
    "print(\"=== Tarifs ===\")\n",
    "print(f\"Mistral OCR  : ${settings.mistral_ocr_cost_per_1000_pages}/1000 pages\")\n",
    "openai_price = settings.openai_pricing.get(settings.default_openai_model, (0, 0))\n",
    "print(f\"OpenAI ({settings.default_openai_model}): ${openai_price[0]}/M input, ${openai_price[1]}/M output\")\n",
    "print()\n",
    "\n",
    "# Estimation des couts\n",
    "pdfs_preview = [p for p in (project_root / \"data\" / \"raw\").glob(\"*.pdf\") if not p.name.startswith(\"ELEVE_TEST\")]\n",
    "if pdfs_preview:\n",
    "    # Estimation OCR\n",
    "    estimate_ocr = estimate_mistral_cost(pdfs_preview)\n",
    "\n",
    "    # Estimation LLM (1 synthese par PDF)\n",
    "    estimate_llm = estimate_synthese_cost(\n",
    "        nb_eleves=len(pdfs_preview),\n",
    "        avg_input_tokens=2000,  # ~bulletin + prompt\n",
    "        avg_output_tokens=500,   # ~synthese JSON\n",
    "    )\n",
    "\n",
    "    print(\"=== Estimation des couts ===\")\n",
    "    print(f\"PDFs a traiter : {len(pdfs_preview)}\")\n",
    "    print(f\"Pages totales  : {estimate_ocr['pages']}\")\n",
    "    print()\n",
    "    print(f\"OCR Mistral    : ${estimate_ocr['cost_usd']:.4f}\")\n",
    "    print(f\"LLM OpenAI     : ${estimate_llm['cost_usd']:.4f} ({estimate_llm['total_tokens']:,} tokens)\")\n",
    "    print(f\"  - Par eleve  : ${estimate_llm['cost_per_eleve']:.6f}\")\n",
    "    print(f\"TOTAL ESTIME   : ${estimate_ocr['cost_usd'] + estimate_llm['cost_usd']:.4f}\")\n",
    "else:\n",
    "    print(\"Aucun PDF trouve dans data/raw/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des modeles\n",
    "\n",
    "- CamemBERT NER pour la detection des noms\n",
    "- Client Mistral pour l'OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from mistralai import Mistral\n",
    "from transformers import pipeline\n",
    "\n",
    "# Charger CamemBERT NER\n",
    "print(f\"Chargement NER: {settings.ner_model}...\")\n",
    "nlp_ner = pipeline(\n",
    "    \"ner\",\n",
    "    model=settings.ner_model,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "print(\"  NER charge\")\n",
    "\n",
    "# Init client Mistral\n",
    "mistral_api_key = os.getenv(\"MISTRAL_API_KEY\") or os.getenv(\"MISTRAL_OCR_API_KEY\")\n",
    "if not mistral_api_key:\n",
    "    raise ValueError(\"MISTRAL_API_KEY ou MISTRAL_OCR_API_KEY requis dans .env\")\n",
    "\n",
    "mistral_client = Mistral(api_key=mistral_api_key)\n",
    "print(f\"  Mistral OCR pret (model: {settings.mistral_ocr_model})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Module d'anonymisation\n",
    "\n",
    "Utilisation directe du module `src.document.anonymizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.document.anonymizer import PDFAnonymizer\n",
    "\n",
    "# Creer l'anonymizer (utilise la config ner_model automatiquement)\n",
    "anonymizer = PDFAnonymizer()\n",
    "print(f\"Anonymizer cree avec NER: {anonymizer._ner_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fonction OCR Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "\n",
    "\n",
    "def extract_with_mistral_ocr(pdf_bytes: bytes) -> dict:\n",
    "    \"\"\"Extrait le contenu d'un PDF avec Mistral OCR.\"\"\"\n",
    "    base64_pdf = base64.b64encode(pdf_bytes).decode(\"utf-8\")\n",
    "\n",
    "    response = mistral_client.ocr.process(\n",
    "        model=settings.mistral_ocr_model,\n",
    "        document={\n",
    "            \"type\": \"document_url\",\n",
    "            \"document_url\": f\"data:application/pdf;base64,{base64_pdf}\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return json.loads(response.model_dump_json())\n",
    "\n",
    "print(\"Fonction OCR definie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pipeline : Anonymisation + OCR\n",
    "\n",
    "Traitement d'un PDF de test (ou du premier PDF disponible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Selectionner un PDF (exclure les fichiers de test)\n",
    "pdfs = [p for p in sorted(RAW_DIR.glob(\"*.pdf\")) if not p.name.startswith(\"ELEVE_TEST\")]\n",
    "\n",
    "if not pdfs:\n",
    "    print(\"Aucun PDF trouve dans data/raw/\")\n",
    "    print(\"Creez un PDF de test ou ajoutez des bulletins.\")\n",
    "else:\n",
    "    pdf_path = pdfs[0]\n",
    "    print(f\"Traitement de: {pdf_path.name}\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etape 1-3: Anonymisation\n",
    "if pdfs:\n",
    "    eleve_id = pdf_path.stem  # Utiliser le nom du fichier comme ID\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    try:\n",
    "        result = anonymizer.anonymize(pdf_path, eleve_id=eleve_id)\n",
    "        duration_anon = time.perf_counter() - start\n",
    "\n",
    "        print(f\"1. Nom extrait     : '{result.identity.get('nom_complet', 'N/A')}'\")\n",
    "        print(f\"2. Variantes NER   : {result.variants_found}\")\n",
    "        print(f\"3. Remplacements   : {result.replacements_count}\")\n",
    "        print(f\"   Duree anonymisation : {duration_anon:.2f}s\")\n",
    "\n",
    "        # Stocker pour les etapes suivantes\n",
    "        pdf_anonymise = result.pdf_bytes\n",
    "        identity = result.identity\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur anonymisation: {e}\")\n",
    "        pdf_anonymise = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etape 4: OCR\n",
    "if pdfs and pdf_anonymise:\n",
    "    start = time.perf_counter()\n",
    "    ocr_result = extract_with_mistral_ocr(pdf_anonymise)\n",
    "    duration_ocr = time.perf_counter() - start\n",
    "\n",
    "    markdown_ocr = ocr_result[\"pages\"][0][\"markdown\"]\n",
    "    print(f\"4. OCR Mistral     : {len(markdown_ocr)} caracteres ({duration_ocr:.2f}s)\")\n",
    "    print()\n",
    "    print(\"--- Apercu du texte OCR (500 premiers chars) ---\")\n",
    "    print(markdown_ocr[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Parsing structure -> EleveExtraction\n",
    "\n",
    "Conversion du markdown OCR en objet `EleveExtraction` structure.\n",
    "\n",
    "**Note**: Cette etape est simplifiee. En production, un parser plus sophistique extraira les matieres, notes, appreciations, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from src.core.models import EleveExtraction, MatiereExtraction\n",
    "\n",
    "\n",
    "def parse_ocr_to_eleve(markdown: str, eleve_id: str, identity: dict) -> EleveExtraction:\n",
    "    \"\"\"Parse le markdown OCR en EleveExtraction.\n",
    "\n",
    "    Version simplifiee - extrait les informations de base.\n",
    "    \"\"\"\n",
    "    # Detecter le genre\n",
    "    genre = identity.get(\"genre\")\n",
    "    if not genre:\n",
    "        if re.search(r\"\\bFille\\b\", markdown, re.IGNORECASE):\n",
    "            genre = \"Fille\"\n",
    "        elif re.search(r\"\\bGarcon\\b|\\bGarÃ§on\\b\", markdown, re.IGNORECASE):\n",
    "            genre = \"Garcon\"\n",
    "\n",
    "    # Detecter la classe\n",
    "    classe_match = re.search(r\"Classe\\s*:\\s*([^\\n]+)\", markdown)\n",
    "    classe = classe_match.group(1).strip() if classe_match else None\n",
    "\n",
    "    # Detecter les absences\n",
    "    absences_match = re.search(r\"(\\d+)\\s*demi-journ\", markdown)\n",
    "    absences = int(absences_match.group(1)) if absences_match else None\n",
    "\n",
    "    # Detecter les retards\n",
    "    retards_match = re.search(r\"(\\d+)\\s*retard\", markdown, re.IGNORECASE)\n",
    "    retards = int(retards_match.group(1)) if retards_match else None\n",
    "\n",
    "    # TODO: Parser les matieres, notes, appreciations\n",
    "    # Pour l'instant, on cree des donnees de demonstration\n",
    "    matieres = [\n",
    "        MatiereExtraction(\n",
    "            nom=\"Mathematiques\",\n",
    "            moyenne_eleve=12.5,\n",
    "            moyenne_classe=11.8,\n",
    "            appreciation=\"Bon trimestre. Travail regulier.\"\n",
    "        ),\n",
    "        MatiereExtraction(\n",
    "            nom=\"Francais\",\n",
    "            moyenne_eleve=14.0,\n",
    "            moyenne_classe=12.3,\n",
    "            appreciation=\"Excellent travail. Participation active.\"\n",
    "        ),\n",
    "        MatiereExtraction(\n",
    "            nom=\"Anglais\",\n",
    "            moyenne_eleve=9.5,\n",
    "            moyenne_classe=11.0,\n",
    "            appreciation=\"Des difficultes persistantes. Doit s'investir davantage.\"\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    return EleveExtraction(\n",
    "        eleve_id=eleve_id,\n",
    "        genre=genre,\n",
    "        classe=classe,\n",
    "        trimestre=1,\n",
    "        absences_demi_journees=absences,\n",
    "        retards=retards,\n",
    "        matieres=matieres,\n",
    "        raw_text=markdown,\n",
    "    )\n",
    "\n",
    "print(\"Fonction de parsing definie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parser le resultat OCR\n",
    "if pdfs and pdf_anonymise:\n",
    "    eleve = parse_ocr_to_eleve(markdown_ocr, eleve_id, identity)\n",
    "\n",
    "    print(\"5. Parsing structure\")\n",
    "    print(f\"   eleve_id : {eleve.eleve_id}\")\n",
    "    print(f\"   genre    : {eleve.genre}\")\n",
    "    print(f\"   classe   : {eleve.classe}\")\n",
    "    print(f\"   absences : {eleve.absences_demi_journees}\")\n",
    "    print(f\"   retards  : {eleve.retards}\")\n",
    "    print(f\"   matieres : {len(eleve.matieres)}\")\n",
    "    for m in eleve.matieres:\n",
    "        print(f\"      - {m.nom}: {m.moyenne_eleve}/20 (classe: {m.moyenne_classe})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stockage DuckDB\n",
    "\n",
    "Initialisation de la base et stockage de l'eleve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.storage.connection import DuckDBConnection\n",
    "from src.storage.repositories.classe import ClasseRepository\n",
    "from src.storage.repositories.eleve import EleveRepository\n",
    "from src.storage.repositories.synthese import SyntheseRepository\n",
    "\n",
    "# Utiliser une DB de test pour le notebook\n",
    "TEST_DB = DB_DIR / \"chiron_notebook.duckdb\"\n",
    "\n",
    "# Initialiser la connexion et creer les tables\n",
    "conn = DuckDBConnection(TEST_DB)\n",
    "conn.ensure_tables()\n",
    "\n",
    "# Creer les repositories\n",
    "eleve_repo = EleveRepository(TEST_DB)\n",
    "synthese_repo = SyntheseRepository(TEST_DB)\n",
    "classe_repo = ClasseRepository(TEST_DB)\n",
    "\n",
    "print(f\"6. DuckDB initialise: {TEST_DB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creer la classe si necessaire\n",
    "if pdfs and eleve:\n",
    "    classe_id = eleve.classe or \"CLASSE_TEST\"\n",
    "\n",
    "    if not classe_repo.get(classe_id):\n",
    "        from src.storage.repositories.classe import Classe\n",
    "        classe_repo.create(Classe(classe_id=classe_id, nom=classe_id))\n",
    "        print(f\"   Classe creee: {classe_id}\")\n",
    "    else:\n",
    "        print(f\"   Classe existante: {classe_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder l'eleve\n",
    "if pdfs and eleve:\n",
    "    # S'assurer que la classe est definie\n",
    "    eleve.classe = classe_id\n",
    "\n",
    "    if eleve_repo.exists(eleve.eleve_id):\n",
    "        print(f\"   Eleve deja existant: {eleve.eleve_id}\")\n",
    "        # Mettre a jour\n",
    "        eleve_repo.update(eleve.eleve_id, matieres=eleve.matieres)\n",
    "        print(\"   -> Mis a jour\")\n",
    "    else:\n",
    "        eleve_repo.create(eleve)\n",
    "        print(f\"   Eleve cree: {eleve.eleve_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifier le stockage\n",
    "if pdfs and eleve:\n",
    "    stored = eleve_repo.get(eleve.eleve_id)\n",
    "    if stored:\n",
    "        print(\"   Verification: OK\")\n",
    "        print(f\"   - {len(stored.matieres)} matieres stockees\")\n",
    "    else:\n",
    "        print(\"   Verification: ERREUR - eleve non trouve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generation de synthese LLM\n",
    "\n",
    "Utilisation du `SyntheseGenerator` pour generer une synthese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.generation.generator import SyntheseGenerator\n",
    "\n",
    "# Creer le generateur (utilise OpenAI par defaut)\n",
    "generator = SyntheseGenerator(\n",
    "    provider=\"openai\",\n",
    "    model=settings.default_openai_model,\n",
    ")\n",
    "\n",
    "print(f\"7. Generateur LLM: {generator.provider}/{generator.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher le prompt qui sera envoye au LLM (debug)\n",
    "if pdfs and eleve:\n",
    "    from src.generation.prompt_builder import format_eleve_data\n",
    "\n",
    "    print(\"--- Donnees eleve formatees pour le LLM ---\")\n",
    "    print(format_eleve_data(eleve))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generer la synthese\n",
    "if pdfs and eleve:\n",
    "    print(\"Generation en cours...\")\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    try:\n",
    "        result = generator.generate_with_metadata(\n",
    "            eleve=eleve,\n",
    "            max_tokens=settings.synthese_max_tokens,\n",
    "        )\n",
    "        duration_llm = time.perf_counter() - start\n",
    "\n",
    "        synthese = result.synthese\n",
    "        metadata = result.metadata\n",
    "\n",
    "        print(f\"   Duree LLM: {duration_llm:.2f}s\")\n",
    "        print(f\"   Tokens: {metadata.get('tokens_total', 'N/A')}\")\n",
    "        print()\n",
    "        print(\"--- Synthese generee ---\")\n",
    "        print(synthese.synthese_texte)\n",
    "        print()\n",
    "        print(f\"Posture: {synthese.posture_generale}\")\n",
    "        print(f\"Alertes: {len(synthese.alertes)}\")\n",
    "        for a in synthese.alertes:\n",
    "            print(f\"  - [{a.severite}] {a.matiere}: {a.description}\")\n",
    "        print(f\"Reussites: {len(synthese.reussites)}\")\n",
    "        for r in synthese.reussites:\n",
    "            print(f\"  - {r.matiere}: {r.description}\")\n",
    "        print(f\"Axes de travail: {synthese.axes_travail}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur generation: {e}\")\n",
    "        synthese = None\n",
    "        metadata = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder la synthese en base\n",
    "if pdfs and synthese:\n",
    "    from src.generation.prompt_builder import format_eleve_data\n",
    "    from src.generation.prompts import CURRENT_PROMPT, get_prompt_hash\n",
    "\n",
    "    # Calculer le hash du prompt pour tracabilite\n",
    "    eleve_data_str = format_eleve_data(eleve)\n",
    "    prompt_hash = get_prompt_hash(CURRENT_PROMPT, eleve_data_str)\n",
    "\n",
    "    # Preparer les metadonnees\n",
    "    db_metadata = {\n",
    "        \"llm_provider\": metadata.get(\"llm_provider\", \"openai\"),\n",
    "        \"llm_model\": metadata.get(\"llm_model\"),\n",
    "        \"llm_response_raw\": metadata.get(\"llm_response_raw\"),\n",
    "        \"prompt_template\": CURRENT_PROMPT,\n",
    "        \"prompt_hash\": prompt_hash,\n",
    "        \"tokens_input\": metadata.get(\"tokens_input\"),\n",
    "        \"tokens_output\": metadata.get(\"tokens_output\"),\n",
    "        \"tokens_total\": metadata.get(\"tokens_total\"),\n",
    "        \"llm_duration_ms\": int(duration_llm * 1000),\n",
    "        \"llm_temperature\": settings.default_temperature,\n",
    "    }\n",
    "\n",
    "    # Creer la synthese\n",
    "    synthese_id = synthese_repo.create(\n",
    "        eleve_id=eleve.eleve_id,\n",
    "        synthese=synthese,\n",
    "        trimestre=eleve.trimestre or 1,\n",
    "        metadata=db_metadata,\n",
    "    )\n",
    "\n",
    "    print(f\"8. Synthese sauvegardee: {synthese_id}\")\n",
    "    print(f\"   prompt_hash: {prompt_hash[:16]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export CSV\n",
    "\n",
    "Generation d'un fichier CSV avec les syntheses validees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marquer comme validee pour pouvoir exporter\n",
    "if pdfs and synthese:\n",
    "    synthese_repo.update_status(synthese_id, \"validated\", \"notebook_test\")\n",
    "    print(f\"Synthese {synthese_id} marquee comme validee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperer les syntheses validees\n",
    "if pdfs:\n",
    "    validated = synthese_repo.get_validated(classe_id, trimestre=1)\n",
    "\n",
    "    print(f\"9. Syntheses validees: {len(validated)}\")\n",
    "\n",
    "    # Generer le CSV\n",
    "    csv_lines = [\"eleve_id;synthese_texte;posture_generale;alertes;reussites\"]\n",
    "\n",
    "    for item in validated:\n",
    "        s = item[\"synthese\"]\n",
    "        text = s.synthese_texte.replace('\"', '\"\"')  # Escape quotes\n",
    "        alertes = \"; \".join(f\"{a.matiere}: {a.description}\" for a in s.alertes)\n",
    "        reussites = \"; \".join(f\"{r.matiere}: {r.description}\" for r in s.reussites)\n",
    "\n",
    "        csv_lines.append(\n",
    "            f'\"{item[\"eleve_id\"]}\";\"{text}\";\"{s.posture_generale}\";\"{alertes}\";\"{reussites}\"'\n",
    "        )\n",
    "\n",
    "    csv_content = \"\\n\".join(csv_lines)\n",
    "\n",
    "    # Sauvegarder\n",
    "    csv_path = DATA_DIR / f\"export_syntheses_{classe_id}_T1.csv\"\n",
    "    csv_path.write_text(csv_content, encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"   Export: {csv_path}\")\n",
    "    print()\n",
    "    print(\"--- Apercu CSV ---\")\n",
    "    print(csv_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Resume du workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pdfs:\n",
    "    # Calculer le cout LLM reel\n",
    "    from src.llm.pricing import PricingCalculator\n",
    "\n",
    "    llm_cost = 0.0\n",
    "    if metadata and metadata.get(\"tokens_input\") and metadata.get(\"tokens_output\"):\n",
    "        calculator = PricingCalculator(\"openai\", settings.openai_pricing)\n",
    "        llm_cost = calculator.calculate(\n",
    "            model=metadata.get(\"llm_model\", settings.default_openai_model),\n",
    "            prompt_tokens=metadata.get(\"tokens_input\", 0),\n",
    "            completion_tokens=metadata.get(\"tokens_output\", 0),\n",
    "        )\n",
    "\n",
    "    # Cout OCR\n",
    "    ocr_cost = 1 / 1000 * settings.mistral_ocr_cost_per_1000_pages  # 1 page\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"RESUME DU WORKFLOW\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"1. PDF source       : {pdf_path.name}\")\n",
    "    print(f\"2. Nom original     : {identity.get('nom_complet', 'N/A')}\")\n",
    "    print(f\"3. ID anonyme       : {eleve_id}\")\n",
    "    print(f\"4. Anonymisation    : {result.replacements_count} remplacements ({duration_anon:.2f}s)\")\n",
    "    print(f\"5. OCR Mistral      : {len(markdown_ocr)} chars ({duration_ocr:.2f}s)\")\n",
    "    print(f\"6. Stockage DuckDB  : {TEST_DB.name}\")\n",
    "    print(f\"7. Generation LLM   : {metadata.get('tokens_total', 'N/A')} tokens ({duration_llm:.2f}s)\")\n",
    "    print(f\"8. Export CSV       : {csv_path.name}\")\n",
    "    print()\n",
    "    print(\"=== COUTS REELS ===\")\n",
    "    print(f\"OCR Mistral (1 page) : ${ocr_cost:.4f}\")\n",
    "    print(f\"LLM OpenAI           : ${llm_cost:.6f}\")\n",
    "    print(f\"  - Input tokens     : {metadata.get('tokens_input', 0):,}\")\n",
    "    print(f\"  - Output tokens    : {metadata.get('tokens_output', 0):,}\")\n",
    "    print(f\"TOTAL                : ${ocr_cost + llm_cost:.4f}\")\n",
    "    print()\n",
    "    print(f\"Temps total: {duration_anon + duration_ocr + duration_llm:.2f}s\")\n",
    "else:\n",
    "    print(\"Aucun PDF traite - ajoutez des PDFs dans data/raw/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer la base de test\n",
    "# Decommenter pour nettoyer:\n",
    "# TEST_DB.unlink(missing_ok=True)\n",
    "# print(\"Base de test supprimee\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
